{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few tasks of NLP\n",
    "1. Part of Speech Tagging\n",
    "2. Parsing - Syntactic / Semantic / Dependency\n",
    "3. Named Entity Relation\n",
    "\n",
    "## Higher Level Tasks\n",
    "1. Natural Language Entailment : Entailment , Contradiction and Neutral : SNLI / MNLI\n",
    "2. Semantic Textual Similarity : Entailment and Not-Entailment : STS\n",
    "3. Machine Comprehension : Question-Answering, Summarization : Squad, Swag\n",
    "4. Machine Translation : English to French , Need to not just be Natural language \n",
    "\n",
    "[https://gluebenchmark.com/leaderboard] (GLUE Leaderboard)\n",
    "\n",
    "[https://rajpurkar.github.io/SQuAD-explorer/] (SQuaD Leaderboard)\n",
    "\n",
    "[https://stanfordnlp.github.io/coqa/] (Conversational QA)\n",
    "\n",
    "## Is this enough? Reasoning and Common Sense, Open Domain QA, Maths ...\n",
    "[https://leaderboard.allenai.org/] (Allen AI Leaderboard)\n",
    "\n",
    "## Summary of Tools\n",
    "\n",
    "\n",
    "1.   Google Colab : GPU and TPUs. File Uploads and Downloads. \n",
    "2.   Jupyter Notebooks\n",
    "3.   PyTorch as Deep Learning Tool\n",
    "4.   AllenNLP as NLP Framework\n",
    "5.   NLTK for Tokenizers, Stopwords, Datasets\n",
    "6.   SpaCy for Tokenizers, POS and DEP taggings, feature generations. KParser and Stanford Parsers\n",
    "7.   AllenNLP Demo\n",
    "8.   Sklearn and GridSearch Demo\n",
    "9.   Embeddings\n",
    "10.  BERT, ELMO , OpenAI , Glove\n",
    "\n",
    "## Resources:\n",
    "1. AWS Educate : https://aws.amazon.com/education/awseducate/ : 100$\n",
    "2. Google GCP : 100$\n",
    "3. Use Spot Instances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Downloader\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Downloader>  l\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Packages:\n",
      "  [ ] abc................. Australian Broadcasting Commission 2006\n",
      "  [ ] alpino.............. Alpino Dutch Treebank\n",
      "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
      "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
      "  [ ] basque_grammars..... Grammars for Basque\n",
      "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
      "                           Extraction Systems in Biology)\n",
      "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
      "  [ ] book_grammars....... Grammars from NLTK Book\n",
      "  [ ] brown............... Brown Corpus\n",
      "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
      "  [ ] cess_cat............ CESS-CAT Treebank\n",
      "  [ ] cess_esp............ CESS-ESP Treebank\n",
      "  [ ] chat80.............. Chat-80 Data Files\n",
      "  [ ] city_database....... City Database\n",
      "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
      "  [ ] comparative_sentences Comparative Sentence Dataset\n",
      "  [ ] comtrans............ ComTrans Corpus Sample\n",
      "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
      "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "!pip install torch\n",
    "!pip install allennlp\n",
    "!pip install --upgrade numpy pandas plotly\n",
    "\n",
    "'''\n",
    "\n",
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp('Apple is looking at buying U.K. startup for $1 billion')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS, NER, DEPENDENCY PARSING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple apple PROPN NNP nsubj Xxxxx True False\n",
      "is be VERB VBZ aux xx True True\n",
      "looking look VERB VBG ROOT xxxx True False\n",
      "at at ADP IN prep xx True True\n",
      "buying buy VERB VBG pcomp xxxx True False\n",
      "U.K. u.k. PROPN NNP compound X.X. False False\n",
      "startup startup NOUN NN dobj xxxx True False\n",
      "for for ADP IN prep xxx True True\n",
      "$ $ SYM $ quantmod $ False False\n",
      "1 1 NUM CD compound d False False\n",
      "billion billion NUM CD pobj xxxx True False\n",
      "\n",
      "Apple 0 5 ORG\n",
      "U.K. 27 31 GPE\n",
      "$1 billion 44 54 MONEY\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)\n",
    "    \n",
    "print()\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity between Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Apple 1.0\n",
      "Apple is -0.05606477\n",
      "Apple looking 0.17363018\n",
      "Apple at 0.09761385\n",
      "Apple buying 0.23062494\n",
      "Apple U.K. 0.49552646\n",
      "Apple startup 0.088767774\n",
      "Apple for -0.020235488\n",
      "Apple $ 0.05591571\n",
      "Apple 1 0.116298646\n",
      "Apple billion 0.1802153\n",
      "is Apple -0.05606477\n",
      "is is 1.0\n",
      "is looking 0.03646119\n",
      "is at 0.15363081\n",
      "is buying -0.010881584\n",
      "is U.K. 0.014506461\n",
      "is startup 0.22643858\n",
      "is for 0.22094187\n",
      "is $ -0.015712954\n",
      "is 1 0.015063022\n",
      "is billion -0.054269664\n",
      "looking Apple 0.17363018\n",
      "looking is 0.03646119\n",
      "looking looking 1.0\n",
      "looking at 0.20644525\n",
      "looking buying 0.55810255\n",
      "looking U.K. 0.010433668\n",
      "looking startup 0.4008714\n",
      "looking for 0.045873784\n",
      "looking $ 0.093066834\n",
      "looking 1 0.089511566\n",
      "looking billion 0.10066204\n",
      "at Apple 0.09761385\n",
      "at is 0.15363081\n",
      "at looking 0.20644525\n",
      "at at 1.0\n",
      "at buying 0.20436426\n",
      "at U.K. 0.07365204\n",
      "at startup 0.13014099\n",
      "at for 0.60098934\n",
      "at $ 0.116372705\n",
      "at 1 0.18446393\n",
      "at billion 0.21969722\n",
      "buying Apple 0.23062494\n",
      "buying is -0.010881584\n",
      "buying looking 0.55810255\n",
      "buying at 0.20436426\n",
      "buying buying 1.0\n",
      "buying U.K. 0.1660859\n",
      "buying startup 0.43678468\n",
      "buying for -0.0004890834\n",
      "buying $ 0.119508125\n",
      "buying 1 -0.0859962\n",
      "buying billion -0.016693909\n",
      "U.K. Apple 0.49552646\n",
      "U.K. is 0.014506461\n",
      "U.K. looking 0.010433668\n",
      "U.K. at 0.07365204\n",
      "U.K. buying 0.1660859\n",
      "U.K. U.K. 1.0\n",
      "U.K. startup 0.0916177\n",
      "U.K. for -0.021887416\n",
      "U.K. $ 0.16066311\n",
      "U.K. 1 0.016652359\n",
      "U.K. billion 0.12517026\n",
      "startup Apple 0.088767774\n",
      "startup is 0.22643858\n",
      "startup looking 0.4008714\n",
      "startup at 0.13014099\n",
      "startup buying 0.43678468\n",
      "startup U.K. 0.0916177\n",
      "startup startup 1.0\n",
      "startup for 0.078537524\n",
      "startup $ -0.16036302\n",
      "startup 1 -0.09009845\n",
      "startup billion 0.030184096\n",
      "for Apple -0.020235488\n",
      "for is 0.22094187\n",
      "for looking 0.045873784\n",
      "for at 0.60098934\n",
      "for buying -0.0004890834\n",
      "for U.K. -0.021887416\n",
      "for startup 0.078537524\n",
      "for for 1.0\n",
      "for $ -0.00040658534\n",
      "for 1 0.06878848\n",
      "for billion 0.18636677\n",
      "$ Apple 0.05591571\n",
      "$ is -0.015712954\n",
      "$ looking 0.093066834\n",
      "$ at 0.116372705\n",
      "$ buying 0.119508125\n",
      "$ U.K. 0.16066311\n",
      "$ startup -0.16036302\n",
      "$ for -0.00040658534\n",
      "$ $ 1.0\n",
      "$ 1 0.33080927\n",
      "$ billion 0.14473616\n",
      "1 Apple 0.116298646\n",
      "1 is 0.015063022\n",
      "1 looking 0.089511566\n",
      "1 at 0.18446393\n",
      "1 buying -0.0859962\n",
      "1 U.K. 0.016652359\n",
      "1 startup -0.09009845\n",
      "1 for 0.06878848\n",
      "1 $ 0.33080927\n",
      "1 1 1.0\n",
      "1 billion 0.632011\n",
      "billion Apple 0.1802153\n",
      "billion is -0.054269664\n",
      "billion looking 0.10066204\n",
      "billion at 0.21969722\n",
      "billion buying -0.016693909\n",
      "billion U.K. 0.12517026\n",
      "billion startup 0.030184096\n",
      "billion for 0.18636677\n",
      "billion $ 0.14473616\n",
      "billion 1 0.632011\n",
      "billion billion 1.0\n"
     ]
    }
   ],
   "source": [
    "for token1 in doc:\n",
    "    for token2 in doc:\n",
    "        print(token1.text, token2.text, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"777-0\" class=\"displacy\" width=\"750\" height=\"312.0\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">This</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">sentence.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-777-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-777-0-0\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-777-0-1\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-777-0-1\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,179.0 L412,167.0 428,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-777-0-2\" stroke-width=\"2px\" d=\"M245,177.0 C245,2.0 575.0,2.0 575.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-777-0-2\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">attr</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M575.0,179.0 L583.0,167.0 567.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5\">When \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Sebastian Thrun\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " started working on self-driving cars at \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Google\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " in \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    2007\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ", few people outside of the company took him seriously.</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "doc_dep = nlp(u'This is a sentence.')\n",
    "displacy.render(doc_dep, style='dep', jupyter=True)\n",
    "\n",
    "doc_ent = nlp(u'When Sebastian Thrun started working on self-driving cars at Google '\n",
    "              u'in 2007, few people outside of the company took him seriously.')\n",
    "displacy.render(doc_ent, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index\tquestion\tsentence\tlabel\n",
      "\n",
      "0\tWhat is the Grotto at Notre Dame?\tImmediately behind the basilica is the Grotto, a Marian place of prayer and reflection.\tentailment\n",
      "\n",
      "1\tWhat is the Grotto at Notre Dame?\tIt is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858.\tnot_entailment\n",
      "\n",
      "2\tWhat sits on top of the Main Building at Notre Dame?\tAtop the Main Building's gold dome is a golden statue of the Virgin Mary.\tentailment\n",
      "\n",
      "3\tWhat sits on top of the Main Building at Notre Dame?\tNext to the Main Building is the Basilica of the Sacred Heart.\tnot_entailment\n",
      "\n"
     ]
    }
   ],
   "source": [
    "qnli = open(\"/scratch/pbanerj6/datasets/glue_data/QNLI/train.tsv\").readlines()[0:5]\n",
    "for line in qnli:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Iterator, List, Dict, Optional\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# for dataset reader\n",
    "from allennlp.data import Instance\n",
    "from allennlp.data.fields import TextField, SequenceLabelField, LabelField\n",
    "from allennlp.data.dataset_readers import DatasetReader\n",
    "from allennlp.common.file_utils import cached_path\n",
    "from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n",
    "from allennlp.data.tokenizers import Token, Tokenizer, WordTokenizer\n",
    "from allennlp.data.vocabulary import Vocabulary\n",
    "\n",
    "# read pretrained embedding from AWS S3\n",
    "from allennlp.modules.token_embedders.embedding import _read_embeddings_from_text_file\n",
    "\n",
    "# for building model\n",
    "from allennlp.models import Model\n",
    "from allennlp.modules.text_field_embedders import TextFieldEmbedder, BasicTextFieldEmbedder\n",
    "from allennlp.modules.token_embedders import Embedding\n",
    "from allennlp.modules.seq2vec_encoders import Seq2VecEncoder, PytorchSeq2VecWrapper\n",
    "from allennlp.modules.seq2seq_encoders import Seq2SeqEncoder, PytorchSeq2SeqWrapper\n",
    "from allennlp.modules import FeedForward\n",
    "from allennlp.nn.util import get_text_field_mask, sequence_cross_entropy_with_logits\n",
    "from allennlp.nn import InitializerApplicator, RegularizerApplicator\n",
    "from allennlp.training.metrics import CategoricalAccuracy\n",
    "from allennlp.data.iterators import BucketIterator\n",
    "from allennlp.training.trainer import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PublicationDatasetReader(DatasetReader):\n",
    "    \"\"\"\n",
    "    DatasetReader for publication and venue dataaet\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 tokenizer: Tokenizer = None,\n",
    "                 token_indexers: Dict[str, TokenIndexer] = None, \n",
    "                 lazy: bool = False,\n",
    "                 train_max_len = 1000,\n",
    "                 val_max_len = 500) -> None:\n",
    "        super().__init__(lazy)\n",
    "        self._tokenizer = tokenizer or WordTokenizer()\n",
    "        self._token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n",
    "        self.train_max_len = train_max_len\n",
    "        self.val_max_len = val_max_len\n",
    "\n",
    "   \n",
    "    def _read(self, file_path: str) -> Iterator[Instance]:\n",
    "        \"\"\"\n",
    "        Read publication and venue dataset in JSON format\n",
    "        \n",
    "        Data is in the following format:\n",
    "            {\"title\": ..., \"paperAbstract\": ..., \"venue\": ...}\n",
    "        \"\"\"\n",
    "        is_train = True if \"train\" in file_path else False\n",
    "        max_len = self.train_max_len if is_train else self.val_max_len\n",
    "        with open(cached_path(file_path), \"r\") as data_file:\n",
    "            lines = data_file.readlines()\n",
    "            for line in lines[0:max_len]:\n",
    "                line = line.strip(\"\\n\")\n",
    "                if not line:\n",
    "                    continue\n",
    "                paper_json = json.loads(line)\n",
    "                title = paper_json['title']\n",
    "                abstract = paper_json['paperAbstract']\n",
    "                venue = paper_json['venue']\n",
    "                yield self.text_to_instance(title, abstract, venue)\n",
    "        \n",
    "    def text_to_instance(self, \n",
    "                         title: str, \n",
    "                         abstract: str, \n",
    "                         venue: str=None) -> Instance:\n",
    "        \"\"\"\n",
    "        Turn title, abstract, and venue to instance\n",
    "        \"\"\"\n",
    "        tokenized_title = self._tokenizer.tokenize(title)\n",
    "        tokenized_abstract = self._tokenizer.tokenize(abstract)\n",
    "        title_field = TextField(tokenized_title, self._token_indexers)\n",
    "        abstract_field = TextField(tokenized_abstract, self._token_indexers)\n",
    "        fields = {'title': title_field, \n",
    "                  'abstract': abstract_field}\n",
    "        if venue is not None:\n",
    "            fields['label'] = LabelField(venue)\n",
    "        return Instance(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AcademicPaperClassifier(Model):\n",
    "    \"\"\"\n",
    "    Model to classify venue based on input title and abstract\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 vocab: Vocabulary,\n",
    "                 text_field_embedder: TextFieldEmbedder,\n",
    "                 title_encoder: Seq2VecEncoder,\n",
    "                 abstract_encoder: Seq2VecEncoder,\n",
    "                 classifier_feedforward: FeedForward,\n",
    "                 initializer: InitializerApplicator = InitializerApplicator(),\n",
    "                 regularizer: Optional[RegularizerApplicator] = None) -> None:\n",
    "        super(AcademicPaperClassifier, self).__init__(vocab, regularizer)\n",
    "        self.text_field_embedder = text_field_embedder\n",
    "        self.num_classes = self.vocab.get_vocab_size(\"labels\")\n",
    "        self.title_encoder = title_encoder\n",
    "        self.abstract_encoder = abstract_encoder\n",
    "        self.classifier_feedforward = classifier_feedforward\n",
    "        self.metrics = {\n",
    "                \"accuracy\": CategoricalAccuracy(),\n",
    "                \"accuracy3\": CategoricalAccuracy(top_k=3)\n",
    "        }\n",
    "        self.loss = torch.nn.CrossEntropyLoss()\n",
    "        initializer(self)\n",
    "    \n",
    "    def forward(self, \n",
    "                title: Dict[str, torch.LongTensor],\n",
    "                abstract: Dict[str, torch.LongTensor],\n",
    "                label: torch.LongTensor = None) -> Dict[str, torch.Tensor]:\n",
    "        \n",
    "        embedded_title = self.text_field_embedder(title)\n",
    "        title_mask = get_text_field_mask(title)\n",
    "        encoded_title = self.title_encoder(embedded_title, title_mask)\n",
    "\n",
    "        embedded_abstract = self.text_field_embedder(abstract)\n",
    "        abstract_mask = get_text_field_mask(abstract)\n",
    "        encoded_abstract = self.abstract_encoder(embedded_abstract, abstract_mask)\n",
    "\n",
    "        logits = self.classifier_feedforward(torch.cat([encoded_title, encoded_abstract], dim=-1))\n",
    "        class_probabilities = F.softmax(logits, dim=-1)\n",
    "        argmax_indices = np.argmax(class_probabilities.cpu().data.numpy(), axis=-1)\n",
    "        labels = [self.vocab.get_token_from_index(x, namespace=\"labels\") for x in argmax_indices]\n",
    "        output_dict = {\n",
    "            'logits': logits, \n",
    "            'class_probabilities': class_probabilities,\n",
    "            'predicted_label': labels\n",
    "        }\n",
    "        if label is not None:\n",
    "            loss = self.loss(logits, label)\n",
    "            for metric in self.metrics.values():\n",
    "                 metric(logits, label)\n",
    "            output_dict[\"loss\"] = loss\n",
    "\n",
    "        return output_dict\n",
    "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
    "        \n",
    "        return {\n",
    "                'accuracy': self.metrics[\"accuracy\"].get_metric(reset),\n",
    "                'accuracy3': self.metrics[\"accuracy3\"].get_metric(reset)\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = \"https://s3-us-west-2.amazonaws.com/allennlp/datasets/academic-papers-example/train.jsonl\"\n",
    "validation_data_path = \"https://s3-us-west-2.amazonaws.com/allennlp/datasets/academic-papers-example/dev.jsonl\"\n",
    "pretrained_file = \"https://s3-us-west-2.amazonaws.com/allennlp/datasets/glove/glove.6B.100d.txt.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PublicationDatasetReader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = reader.text_to_instance(\"This is a great paper.\", \n",
    "                                   \"Indeed, this is a great paper of all time\", \n",
    "                                   \"Nature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1000it [00:02, 401.17it/s]\n",
      "500it [00:01, 438.67it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = reader.read(train_data_path,)\n",
    "validation_dataset = reader.read(validation_data_path,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1500/1500 [00:00<00:00, 3679.47it/s]\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocabulary.from_instances(train_dataset + validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:03, 120373.42it/s]\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = _read_embeddings_from_text_file(file_uri=pretrained_file, \n",
    "                                                   embedding_dim=100, \n",
    "                                                   vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([17398, 100])\n"
     ]
    }
   ],
   "source": [
    "print(embedding_matrix.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 100\n",
    "num_classes = len(vocab.get_index_to_token_vocabulary('labels'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding\n",
    "token_embedding = Embedding(num_embeddings=vocab.get_vocab_size('tokens'), \n",
    "                            embedding_dim=EMBEDDING_DIM,\n",
    "                            weight=embedding_matrix)\n",
    "word_embeddings = BasicTextFieldEmbedder({\"tokens\": token_embedding})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lstm_title = PytorchSeq2VecWrapper(torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, \n",
    "                                                 batch_first=True, bidirectional=True))\n",
    "lstm_abstract = PytorchSeq2VecWrapper(torch.nn.LSTM(EMBEDDING_DIM, HIDDEN_DIM, \n",
    "                                                    batch_first=True, bidirectional=True))\n",
    "feed_forward = torch.nn.Linear(2 * 2 * HIDDEN_DIM, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AcademicPaperClassifier(vocab,\n",
    "                                word_embeddings, \n",
    "                                lstm_title, \n",
    "                                lstm_abstract, \n",
    "                                feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterator = BucketIterator(batch_size=64, \n",
    "                          sorting_keys=[(\"abstract\", \"num_tokens\"), \n",
    "                                        (\"title\", \"num_tokens\")])\n",
    "iterator.index_with(vocab) # index with the created vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    iterator=iterator,\n",
    "    train_dataset=train_dataset,\n",
    "    validation_dataset=validation_dataset,\n",
    "    patience=2,\n",
    "    num_epochs=3,\n",
    "    serialization_dir='output2',\n",
    "    validation_metric=\"+accuracy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training_duration': '00:00:00',\n",
       " 'training_start_epoch': 3,\n",
       " 'training_epochs': 0,\n",
       " 'best_epoch': 2}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'logits': [0.13181300461292267, -0.05287451297044754, -0.06042443588376045], 'class_probabilities': [0.3764386773109436, 0.3129575848579407, 0.31060364842414856], 'predicted_label': 'ACL'}\n"
     ]
    }
   ],
   "source": [
    "from allennlp.common.util import JsonDict\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "\n",
    "class PaperClassifierPredictor(Predictor):\n",
    "    \"\"\"\"\n",
    "    Predictor wrapper for the AcademicPaperClassifier\n",
    "    \"\"\"\n",
    "    def _json_to_instance(self, json_dict: JsonDict) -> Instance:\n",
    "        title = json_dict['title']\n",
    "        abstract = json_dict['paperAbstract']\n",
    "        instance = self._dataset_reader.text_to_instance(title=title, abstract=abstract)\n",
    "        return instance\n",
    "predictor = PaperClassifierPredictor(model, dataset_reader=reader)   \n",
    "prediction_output = predictor.predict_json(\n",
    "    {\n",
    "        \"title\": \"Know What You Don't Know: Unanswerable Questions for SQuAD\", \n",
    "        \"paperAbstract\": \"Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQuAD 2.0, the latest version of the Stanford Question Answering Dataset (SQuAD). SQuAD 2.0 combines existing SQuAD data with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuAD 2.0, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuAD 2.0 is a challenging natural language understanding task for existing models: a strong neural system that gets 86% F1 on SQuAD 1.1 achieves only 66% F1 on SQuAD 2.0.\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(prediction_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Link to Structured GitHub Repo](https://github.com/allenai/allennlp-as-a-library-example)\n",
    "\n",
    "\n",
    "2. [Link to Notebook with Comments](https://github.com/titipata/allennlp-tutorial/blob/master/allennlp_tutorial.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
