{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HuggingFace.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "QAKkPmVLdZ2C",
        "aa1IXRK-VFgu",
        "GzVMYXlpk49W",
        "nDB3oF8ycZaB",
        "HrJJvjJ3cb10",
        "BreB-d8Tc5eQ",
        "qstJCnEvc7CE",
        "l3tBA6Or1soN"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "105b236d43fd486ba04b449e05ac10d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9ac2037bab6548d9b88a456b3beb8a43",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_61ad4070b6e84a3fb5bc8b4409118f79",
              "IPY_MODEL_0ded9d663d324d27ad4275dd8902c209"
            ]
          }
        },
        "9ac2037bab6548d9b88a456b3beb8a43": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "61ad4070b6e84a3fb5bc8b4409118f79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d2ee62a022d148188b4c724ddd592944",
            "_dom_classes": [],
            "description": "  8%",
            "_model_name": "IntProgressModel",
            "bar_style": "",
            "max": 677,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 52,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7e4b3760a17440958fd0c44421904952"
          }
        },
        "0ded9d663d324d27ad4275dd8902c209": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_26b3fca528e048199e5cd08c3dc8eb54",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 52/677 [00:20&lt;00:15, 40.67it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6a83d12cfe2a48fd85cdadff97e1ed53"
          }
        },
        "d2ee62a022d148188b4c724ddd592944": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7e4b3760a17440958fd0c44421904952": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "26b3fca528e048199e5cd08c3dc8eb54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6a83d12cfe2a48fd85cdadff97e1ed53": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cd78f26b426743dc9b090f067e34bac7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_760d52ff1ba34d59baf0c54c6b84c4e0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_407ac0f7d6db459fbd99e6524c6da612",
              "IPY_MODEL_49b8f4f690954034bf0189b544676d61"
            ]
          }
        },
        "760d52ff1ba34d59baf0c54c6b84c4e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "407ac0f7d6db459fbd99e6524c6da612": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_148c503473ba4ea5a28f51115027e4a4",
            "_dom_classes": [],
            "description": " 41%",
            "_model_name": "IntProgressModel",
            "bar_style": "",
            "max": 11724,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 4846,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b99e7cf0fde5460b9407b49495c2444f"
          }
        },
        "49b8f4f690954034bf0189b544676d61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_51bd902df1df46ee9466d3f99f78bf32",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 4846/11724 [55:45&lt;44:54,  2.55it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b56cce81e2f344ef80d460085a958b04"
          }
        },
        "148c503473ba4ea5a28f51115027e4a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b99e7cf0fde5460b9407b49495c2444f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "51bd902df1df46ee9466d3f99f78bf32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b56cce81e2f344ef80d460085a958b04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7a_-c5fvbWa",
        "colab_type": "code",
        "outputId": "fd7385d0-46bd-4a7c-bb9c-105246501e8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.8.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.47)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.86)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.47 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.47)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.47->boto3->transformers) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.47->boto3->transformers) (0.15.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3MLqgPywahc",
        "colab_type": "code",
        "outputId": "2582f9f7-99de-424c-c378-dc6fcc14c4b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxHwnOz-wElJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from transformers import *\n",
        "import numpy as np\n",
        "import scipy as scipy\n",
        "import pandas as pd\n",
        "import os\n",
        "import ast\n",
        "import tqdm as tqdm\n",
        "import spacy\n",
        "\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DdJNWG3X01oV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MODEL_CLASS, TOKENIZER_CLASS, PRETRAINED = (DistilBertForSequenceClassification, DistilBertTokenizer, 'distilbert-base-cased')\n",
        "PAWS_QQP = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnRskxKRiD9l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SPACY_CORE = spacy.load(\"en_core_web_sm\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAKkPmVLdZ2C",
        "colab_type": "text"
      },
      "source": [
        "# **Generate QQP**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDWBK6aedjR6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_path = '/content/drive/My Drive/Colab/data_qqp/'\n",
        "\n",
        "all_data = pd.read_table(os.path.expanduser(data_path + 'quora_duplicate_questions.tsv'), header=0)\n",
        "all_data = all_data.rename(columns={'is_duplicate': 'label', 'question1': 'sentence1', 'question2': 'sentence2'})\n",
        "\n",
        "class_1 = all_data[all_data['label']==1]\n",
        "class_0 = all_data[all_data['label']==0]\n",
        "\n",
        "class_1_dev_test = class_1.sample(n=10000)\n",
        "class_1_train = class_1.drop(class_1_dev_test.index)\n",
        "\n",
        "class_0_dev_test = class_0.sample(n=10000)\n",
        "class_0_train = class_0.drop(class_0_dev_test.index)\n",
        "\n",
        "train_data = pd.concat([class_1_train, class_0_train])\n",
        "train_data = train_data.sample(frac=1).reset_index(drop=True)\n",
        "test_data = pd.concat([class_1_dev_test[:5000], class_0_dev_test[:5000]])\n",
        "test_data = test_data.sample(frac=1).reset_index(drop=True)\n",
        "dev_data = pd.concat([class_1_dev_test[5000:], class_0_dev_test[5000:]])\n",
        "dev_data = dev_data.sample(frac=1).reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMFkl-CZhP7B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data.to_csv(data_path + 'train.tsv', sep='\\t')\n",
        "dev_data.to_csv(data_path + 'dev.tsv', sep='\\t')\n",
        "test_data.to_csv(data_path + 'test.tsv', sep='\\t')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BPg3_DtVIYJ",
        "colab_type": "text"
      },
      "source": [
        "# **Preprocess Data** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0EVyYE8L-CP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_FOLDER = 'data_PAWS_qqp'\n",
        "FILE_NAMES = ['dev.tsv']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dI0CzB3p0uT5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Tokenizer:\n",
        "    # init\n",
        "    def __init__(self, tokenizer_class, pretrained_weights):\n",
        "        self.tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
        "        \n",
        "    # tokenize\n",
        "    def tokenize_data(self, data, file_path, group_sent=True):\n",
        "        bar = tqdm.notebook.tqdm(total=len(data))\n",
        "        tokenized = None\n",
        "        if group_sent == False:\n",
        "            tokenized = pd.DataFrame(columns = ['i1', 's1', 'a1', 'n11', 'n12', 'm1', 'i2', 's2', 'a2', 'n21', 'n22', 'm2', 'y'])\n",
        "        else:\n",
        "            tokenized = pd.DataFrame(columns = ['i', 's', 'a', 'y'])\n",
        "        count = 0\n",
        "        flag = True\n",
        "        i = 0\n",
        "        for index, row in data.iterrows():\n",
        "            token_row = {}\n",
        "            # print(row)\n",
        "            # QQP Dataset\n",
        "            # sent1 = str(row['question1'])\n",
        "            # sent2 = str(row['question2'])\n",
        "            # token_row['y'] = int(row['is_duplicate'])\n",
        "            \n",
        "            # PAWS Dataset\n",
        "            sent1 = str(row['sentence1'])\n",
        "            sent2 = str(row['sentence2'])\n",
        "            token_row['y'] = int(row['label\\r'])\n",
        "            # print(sent1)\n",
        "            # print(sent2)\n",
        "            # print(row['label\\r'])\n",
        "            # if count / 10000 < 14:\n",
        "            #     count += 1\n",
        "            #     continue\n",
        "            \n",
        "            if group_sent == True:\n",
        "                encoding = self.tokenizer.encode_plus(sent1, sent2, return_token_type_ids=True, max_length=256, pad_to_max_length=True)\n",
        "                token_row['i'] = encoding['input_ids']\n",
        "                token_row['s'] = encoding['token_type_ids']\n",
        "                token_row['a'] = encoding['attention_mask']\n",
        "              \n",
        "            else:\n",
        "                encoding1 = self.tokenizer.encode_plus(sent1, return_token_type_ids=True, max_length=128, pad_to_max_length=True)\n",
        "\n",
        "                token_row['i1'] = encoding1['input_ids']\n",
        "                token_row['s1'] = encoding1['token_type_ids']\n",
        "                token_row['a1'] = encoding1['attention_mask']\n",
        "\n",
        "                starts1 = np.zeros(10, dtype=int)\n",
        "                ends1 = np.ones(10, dtype=int)\n",
        "                mask1 = np.zeros(10, dtype=int)\n",
        "                doc1 = SPACY_CORE(sent1)\n",
        "                j = 0\n",
        "                for chunk in doc1.noun_chunks:\n",
        "                    # print(chunk)\n",
        "                    starts1[j] = chunk.start\n",
        "                    ends1[j] = chunk.end\n",
        "                    mask1[j] = 1\n",
        "                    j += 1\n",
        "                    if j == 10:\n",
        "                        break\n",
        "\n",
        "                token_row['n11'] = starts1\n",
        "                token_row['n12'] = ends1\n",
        "                token_row['m1'] = mask1\n",
        "\n",
        "                encoding2 = self.tokenizer.encode_plus(sent2, return_token_type_ids=True, max_length=128, pad_to_max_length=True)\n",
        "\n",
        "                token_row['i2'] = encoding2['input_ids']\n",
        "                token_row['s2'] = encoding2['token_type_ids']\n",
        "                token_row['a2'] = encoding2['attention_mask']\n",
        "\n",
        "                # print(sent2)\n",
        "                starts2 = np.zeros(10, dtype=int)\n",
        "                ends2 = np.ones(10, dtype=int)\n",
        "                mask2 = np.zeros(10, dtype=int)\n",
        "                doc2 = SPACY_CORE(sent2)\n",
        "                j = 0\n",
        "                for chunk in doc2.noun_chunks:\n",
        "                    # print(chunk)\n",
        "                    starts2[j] = chunk.start\n",
        "                    ends2[j] = chunk.end\n",
        "                    mask2[j] = 1\n",
        "                    j += 1\n",
        "                    if j == 10:\n",
        "                        break\n",
        "\n",
        "                token_row['n21'] = starts2\n",
        "                token_row['n22'] = ends2\n",
        "                token_row['m2'] = mask2\n",
        "\n",
        "            tokenized = tokenized.append(token_row, ignore_index=True)\n",
        "            bar.update()\n",
        "            count += 1\n",
        "            if count % 10000 == 0:\n",
        "                tokenized.to_parquet(file_path + str(i) + '.parquet')\n",
        "                tokenized = tokenized.iloc[0:0]\n",
        "                i+=1\n",
        "        if count % 10000 != 0:\n",
        "            tokenized.to_parquet(file_path + str(i) + '.parquet')\n",
        "        bar.close()\n",
        "        return tokenized"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_9WZx3Jwwjd",
        "colab_type": "code",
        "outputId": "f6edce10-35a7-496b-d7aa-30c46b9d2a60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "105b236d43fd486ba04b449e05ac10d2",
            "9ac2037bab6548d9b88a456b3beb8a43",
            "61ad4070b6e84a3fb5bc8b4409118f79",
            "0ded9d663d324d27ad4275dd8902c209",
            "d2ee62a022d148188b4c724ddd592944",
            "7e4b3760a17440958fd0c44421904952",
            "26b3fca528e048199e5cd08c3dc8eb54",
            "6a83d12cfe2a48fd85cdadff97e1ed53"
          ]
        }
      },
      "source": [
        "data_path = '/content/drive/My Drive/Colab/' + DATA_FOLDER + '/'\n",
        "tokenizer = Tokenizer(TOKENIZER_CLASS, PRETRAINED)\n",
        "\n",
        "for file_name in FILE_NAMES:\n",
        "    data = pd.read_csv(os.path.expanduser(data_path+file_name), sep='\\t', lineterminator='\\n', error_bad_lines=False)\n",
        "    data = data.dropna()\n",
        "    tokens = tokenizer.tokenize_data(data, data_path + file_name.split('.')[0] + '_seperate', group_sent=False)\n",
        "    # tokens = tokenizer.tokenize_data(data, data_path + file_name.split('.')[0] + '_grouped')\n",
        "    # tokens.to_csv(data_path + file_name.split('.')[0] + '.csv')\n",
        "    # display(tokens)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "105b236d43fd486ba04b449e05ac10d2",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=677), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "What were the major effects of the cambodia earthquake , and how do these effects compare to the Iquique earthquake in 1877 ?\n",
            "0\n",
            "What\n",
            "the major effects\n",
            "the cambodia earthquake\n",
            "these effects\n",
            "the Iquique earthquake\n",
            "What were the major effects of the Iquique earthquake , and how do these effects compare to the cambodia earthquake in 1877 ?\n",
            "What\n",
            "the major effects\n",
            "the Iquique earthquake\n",
            "these effects\n",
            "the cambodia earthquake\n",
            "The guy I 'm dating never texts me and I feel like he does n't care about me but when I see him he shows me he likes me and wants me . Why do I feel this way ?\n",
            "0\n",
            "The guy\n",
            "I\n",
            "me\n",
            "I\n",
            "he\n",
            "me\n",
            "I\n",
            "him\n",
            "he\n",
            "me\n",
            "The guy I 'm dating never wants me and I feel like he does n't care about me but when I see him , he shows me he likes me and texts me . Why do I feel this way ?\n",
            "The guy\n",
            "me\n",
            "I\n",
            "he\n",
            "me\n",
            "I\n",
            "him\n",
            "he\n",
            "me\n",
            "he\n",
            "How do I make my new phone number as group admin when I do n't have access to my old number ( which is the group admin at present ) on whatsapp ?\n",
            "0\n",
            "I\n",
            "my new phone number\n",
            "group admin\n",
            "I\n",
            "access\n",
            "my old number\n",
            "the group admin\n",
            "whatsapp\n",
            "How do I make my old phone number as group admin when I do n't have access to my new number ( which is the group admin at present ) on whatsapp ?\n",
            "I\n",
            "my old phone number\n",
            "group admin\n",
            "I\n",
            "access\n",
            "my new number\n",
            "the group admin\n",
            "whatsapp\n",
            "Why ca n't countries afford high quality products , but insist to buy low quality Chinese products which Chinese themselves do not want to buy ?\n",
            "0\n",
            "countries\n",
            "high quality products\n",
            "low quality Chinese products\n",
            "Chinese\n",
            "themselves\n",
            "Why ca n't countries afford China 's high quality products , but insist to buy low quality Chinese products which Chinese themselves do not want to buy ?\n",
            "countries\n",
            "China 's high quality products\n",
            "low quality Chinese products\n",
            "Chinese\n",
            "themselves\n",
            "Do Mexican women like East Asian men ( Korean , Japanese , Chinese ) ?\n",
            "0\n",
            "Mexican women\n",
            "East Asian men\n",
            "( Korean , Japanese , Chinese\n",
            "Do East Asian women like Mexican men ( Korean , Japanese , Chinese ) ?\n",
            "East Asian women\n",
            "Mexican men\n",
            "( Korean , Japanese , Chinese\n",
            "I feel as though for 95 % of my day , my mind is in a thinking state , not actively cruising or processing . What is this , what causes this , and how can I get rid of it ?\n",
            "0\n",
            "I\n",
            "95 %\n",
            "my day\n",
            "my mind\n",
            "a thinking state\n",
            "What\n",
            "what\n",
            "I\n",
            "it\n",
            "I feel as though for 95 % of my day , my mind is in a cruising state , not actively thinking or processing . What is this , what causes this , and how can I get rid of it ?\n",
            "I\n",
            "95 %\n",
            "my day\n",
            "my mind\n",
            "a cruising state\n",
            "What\n",
            "what\n",
            "I\n",
            "it\n",
            "What if a girl who ignores me suddenly likes me ? What does it mean ?\n",
            "0\n",
            "What\n",
            "a girl\n",
            "who\n",
            "me\n",
            "me\n",
            "What\n",
            "it\n",
            "What if a girl who likes me suddenly ignores me ? What does it mean ?\n",
            "a girl\n",
            "who\n",
            "me\n",
            "me\n",
            "What\n",
            "it\n",
            "What do British universities require from the German curriculum for medicine ?\n",
            "0\n",
            "What\n",
            "British universities\n",
            "the German curriculum\n",
            "medicine\n",
            "What do German universities require from British curriculum for medicine ?\n",
            "What\n",
            "German universities\n",
            "British curriculum\n",
            "medicine\n",
            "How can I join the Gopichand Badminton Academy ? Will he agree to train me at age 20 ?\n",
            "0\n",
            "I\n",
            "the Gopichand Badminton Academy\n",
            "he\n",
            "me\n",
            "age\n",
            "How can I train the Gopichand Badminton Academy ? Will he agree to join me at age 20 ?\n",
            "I\n",
            "the Gopichand Badminton Academy\n",
            "he\n",
            "me\n",
            "age\n",
            "What are the driving rules in Georgia versus Mississippi ?\n",
            "1\n",
            "What\n",
            "the driving rules\n",
            "Georgia\n",
            "Mississippi\n",
            "What are the driving rules in Mississippi versus Georgia ?\n",
            "What\n",
            "the driving rules\n",
            "Mississippi\n",
            "Georgia\n",
            "Does AT & T have any specific plans for DirecTV 's ROOT Sports networks , post acquisition ?\n",
            "0\n",
            "AT\n",
            "T\n",
            "any specific plans\n",
            "DirecTV 's ROOT Sports networks\n",
            "post acquisition\n",
            "Does DirecTV have any specific plans for AT & T 's ROOT Sports networks , post acquisition ?\n",
            "DirecTV\n",
            "any specific plans\n",
            "AT & T 's ROOT Sports networks\n",
            "post acquisition\n",
            "How do I start learning electronic music production ? Should I begin with books or videos ? I have logic pro 9 and I already made some songs but I would like to start from scratch ( as if I did not know anything ) .\n",
            "1\n",
            "I\n",
            "electronic music production\n",
            "I\n",
            "books\n",
            "videos\n",
            "I\n",
            "logic\n",
            "I\n",
            "some songs\n",
            "I\n",
            "How do I begin learning electronic music production ? Should I start with books or videos ? I have logic pro 9 and I already made some songs but I would like to start from scratch ( as if I did not know anything ) .\n",
            "I\n",
            "electronic music production\n",
            "I\n",
            "books\n",
            "videos\n",
            "I\n",
            "logic\n",
            "I\n",
            "some songs\n",
            "I\n",
            "Can energy be borrowed from vacuum ( to be returned immediately ) ? If this happens how do we know vacuum energy is real if virtual particles do n't exist ?\n",
            "0\n",
            "energy\n",
            "vacuum\n",
            "we\n",
            "vacuum energy\n",
            "virtual particles\n",
            "Can energy be returned from vacuum ( to be borrowed immediately ) , if this happens how do we know vacuum energy is real if virtual particles do n't exist ?\n",
            "energy\n",
            "vacuum\n",
            "we\n",
            "vacuum energy\n",
            "virtual particles\n",
            "`` I had a Google APM interview , and at the end of three interviews they said they had `` enough information. `` None of these interviews were technical . Is the number of interviews you have onsite indicative of your performance ? ''\n",
            "0\n",
            "I\n",
            "a Google APM interview\n",
            "the end\n",
            "three interviews\n",
            "they\n",
            "they\n",
            "` enough information\n",
            "None\n",
            "these interviews\n",
            "the number\n",
            "`` I had a Google APM interview , and at the end of three interviews they said they had `` technical information. `` None of these interviews were enough . Is the number of interviews you have onsite indicative of your performance ? ''\n",
            "I\n",
            "a Google APM interview\n",
            "the end\n",
            "three interviews\n",
            "they\n",
            "they\n",
            "` technical information\n",
            "None\n",
            "these interviews\n",
            "the number\n",
            "How do you drive from Auckland to Edmonton , and how do notable features of the route compare with those of other Canadian cities ?\n",
            "0\n",
            "you\n",
            "Auckland\n",
            "Edmonton\n",
            "notable features\n",
            "the route\n",
            "other Canadian cities\n",
            "How do you drive from Edmonton to Auckland , and how do notable features of the route compare with those of other Canadian cities ?\n",
            "you\n",
            "Edmonton\n",
            "Auckland\n",
            "notable features\n",
            "the route\n",
            "other Canadian cities\n",
            "Which business idea I can get from small city like ( Pune ) & start in metro city ( Gwalior ) ?\n",
            "0\n",
            "Which business idea\n",
            "I\n",
            "small city\n",
            "( Pune\n",
            "metro city\n",
            "( Gwalior\n",
            "Which business idea I can get from metro city like ( Pune ) & start in small city ( Gwalior ) ?\n",
            "Which business idea\n",
            "I\n",
            "metro city\n",
            "( Pune\n",
            "small city\n",
            "( Gwalior\n",
            "What is the best site to integrate my business blog and post with all traders worldwide ?\n",
            "0\n",
            "What\n",
            "the best site\n",
            "my business blog\n",
            "all traders\n",
            "What is the best site to post my business blog and integrate with all traders worldwide ?\n",
            "What\n",
            "the best site\n",
            "my business blog\n",
            "all traders\n",
            "I want to start learn . how should I coding it ?\n",
            "0\n",
            "I\n",
            "I\n",
            "it\n",
            "I want to learn coding how should I start it ?\n",
            "I\n",
            "I\n",
            "it\n",
            "`` Does the fact that Obama called Bernie a `` Shvartza Goy `` prove that Bernie is racist ? ''\n",
            "0\n",
            "the fact\n",
            "Obama\n",
            "Bernie\n",
            "Bernie\n",
            "`` Does the fact that Bernie called Obama a `` Shvartza Goy `` prove that Bernie is racist ? ''\n",
            "the fact\n",
            "Bernie\n",
            "Obama\n",
            "Bernie\n",
            "What are the differences between abiotic factors and biotic factors ?\n",
            "1\n",
            "What\n",
            "the differences\n",
            "abiotic factors\n",
            "biotic factors\n",
            "What are the differences between biotic factors and abiotic factors ?\n",
            "What\n",
            "the differences\n",
            "biotic factors\n",
            "abiotic factors\n",
            "How is the categoricity of the second analysis demonstrated in tensorial-order logic ?\n",
            "0\n",
            "the categoricity\n",
            "the second analysis\n",
            "tensorial-order logic\n",
            "How is the categoricity of the tensorial analysis demonstrated in second-order logic ?\n",
            "the categoricity\n",
            "the tensorial analysis\n",
            "second-order logic\n",
            "`` What does the application status `` Reviewed , not selected `` mean ? ''\n",
            "0\n",
            "What\n",
            "the application status\n",
            "`` What does the application status `` selected , not Reviewed `` mean ? ''\n",
            "What\n",
            "the application status\n",
            "What 's classy if you 're poor , but trashy if you 're rich ?\n",
            "0\n",
            "What\n",
            "you\n",
            "you\n",
            "What 's classy if you 're rich , but trashy if you 're poor ?\n",
            "What\n",
            "you\n",
            "you\n",
            "I 'm currently working for a Big 4 as an Advanced Tax Analyst . The company is willing to sponsor my CPA in the future . Should I continue this job and do CPA or I should leave this job and start preparing for CA , which I feel have much better scope than a CPA ?\n",
            "0\n",
            "I\n",
            "'m\n",
            "an Advanced Tax Analyst\n",
            "The company\n",
            "my CPA\n",
            "the future\n",
            "I\n",
            "this job\n",
            "CPA\n",
            "I\n",
            "I 'm currently working for a Big 4 as an Advanced Tax Analyst . The company is willing to sponsor my CPA in the future . Should I leave this job and do CPA or I should continue this job , and start preparing for CA , which I feel have much better scope than a CPA ?\n",
            "I\n",
            "'m\n",
            "an Advanced Tax Analyst\n",
            "The company\n",
            "my CPA\n",
            "the future\n",
            "I\n",
            "this job\n",
            "CPA\n",
            "I\n",
            "What is actual meaning of life ? Indeen , it depend on perception of people or other thing ?\n",
            "0\n",
            "What\n",
            "actual meaning\n",
            "life\n",
            "it\n",
            "perception\n",
            "people\n",
            "other thing\n",
            "What is other meaning of life ? Indeen , it depend on perception of people or actual thing ?\n",
            "What\n",
            "other meaning\n",
            "life\n",
            "it\n",
            "perception\n",
            "people\n",
            "actual thing\n",
            "What is the relationship among Helsingor , Helsingfors , Helsinki and Helsingborg ?\n",
            "1\n",
            "What\n",
            "the relationship\n",
            "Helsingor\n",
            "Helsingfors\n",
            "Helsinki\n",
            "Helsingborg\n",
            "What is the relationship among Helsinki , Helsingfors , Helsingor and Helsingborg ?\n",
            "What\n",
            "the relationship\n",
            "Helsinki\n",
            "Helsingfors\n",
            "Helsingor\n",
            "Helsingborg\n",
            "In Nevada when pulling out of a parking lot , can you make a right turn into the far left lane if intending to turn into a nearby driveway or business ?\n",
            "0\n",
            "Nevada\n",
            "a parking lot\n",
            "you\n",
            "a right turn\n",
            "the far left lane\n",
            "a nearby driveway\n",
            "business\n",
            "In Nevada when pulling out of a parking lot , can you make a left turn into the far right lane if intending to turn into a nearby driveway or business ?\n",
            "Nevada\n",
            "a parking lot\n",
            "you\n",
            "a left turn\n",
            "the far right lane\n",
            "a nearby driveway\n",
            "business\n",
            "How safe are Indian Rs 500 and Rs 2000 new currency notes ?\n",
            "1\n",
            "Indian Rs\n",
            "new currency\n",
            "How safe is new Rs 500 and Rs 2000 Indian currency notes ?\n",
            "new Rs\n",
            "Indian currency\n",
            "As space expands , it releases stored up intrinsic energy , which converts into the gravitational potential energy that fills the newly created volume ?\n",
            "0\n",
            "space\n",
            "it\n",
            "intrinsic energy\n",
            "the gravitational potential energy\n",
            "the newly created volume\n",
            "As space expands , it releases stored up gravitational potential energy , which converts into the intrinsic energy that fills the newly created volume ?\n",
            "space\n",
            "it\n",
            "gravitational potential energy\n",
            "the intrinsic energy\n",
            "the newly created volume\n",
            "Why do some people write with their left hand and some write with their right hand ?\n",
            "1\n",
            "some people\n",
            "their left hand\n",
            "their right hand\n",
            "Why do some people write with their right hand and some write with their left hand ?\n",
            "some people\n",
            "their right hand\n",
            "their left hand\n",
            "What are the reasons for SAARC 's failure as regional organization for regional integration , economic cooperation and platform to raise common issues ?\n",
            "0\n",
            "What\n",
            "the reasons\n",
            "SAARC 's failure\n",
            "regional organization\n",
            "regional integration\n",
            "economic cooperation\n",
            "platform\n",
            "common issues\n",
            "What are the reasons for SAARC 's failure as common organization and platform to raise common issues for economic integration , regional cooperation ?\n",
            "What\n",
            "the reasons\n",
            "SAARC 's failure\n",
            "common organization\n",
            "platform\n",
            "common issues\n",
            "economic integration\n",
            "regional cooperation\n",
            "What is the difference between active immunity and passive immunity ? What are its similarities ?\n",
            "1\n",
            "What\n",
            "the difference\n",
            "active immunity\n",
            "passive immunity\n",
            "What\n",
            "its similarities\n",
            "What is the difference between passive immunity and active immunity ? What are its similarities ?\n",
            "What\n",
            "the difference\n",
            "passive immunity\n",
            "active immunity\n",
            "What\n",
            "its similarities\n",
            "Do Chinese/Korean people think they look like Japanese people ?\n",
            "0\n",
            "Chinese/Korean people\n",
            "they\n",
            "Japanese people\n",
            "Do Japanese people think they look like Chinese/Korean people ?\n",
            "Japanese people\n",
            "they\n",
            "Chinese/Korean people\n",
            "How do I change my SBI register mobile number ?\n",
            "0\n",
            "I\n",
            "my SBI\n",
            "mobile number\n",
            "How do I register my SBI change mobile number ?\n",
            "I\n",
            "my SBI change mobile number\n",
            "What if Hillary Clinton would have defeated Donald Trump ?\n",
            "0\n",
            "What\n",
            "Hillary Clinton\n",
            "Donald Trump\n",
            "What if Donald Trump would have defeated Hillary Clinton ?\n",
            "What\n",
            "Donald Trump\n",
            "Hillary Clinton\n",
            "How do I integrate quickblox step by step in ios app in swift also the sample swift code showing me 404 error after entering the credentials ?\n",
            "0\n",
            "I\n",
            "quickblox step\n",
            "step\n",
            "ios app\n",
            "swift\n",
            "the sample swift code\n",
            "me\n",
            "404 error\n",
            "the credentials\n",
            "How do I integrate swift step by step in ios app in swift also the sample quick box code showing me 404 error after entering the credentials ?\n",
            "I\n",
            "swift step\n",
            "step\n",
            "ios app\n",
            "swift\n",
            "the sample quick box code\n",
            "me\n",
            "404 error\n",
            "the credentials\n",
            "`` Are `` if I were ... `` and `` if I was ... `` both grammatically correct ? ''\n",
            "1\n",
            "I\n",
            "I\n",
            "Are `` if I was '' and `` if I were '' both grammatically correct ?\n",
            "I\n",
            "I\n",
            "Do Go-Back-N ARQ and Selective Repeat ARQ use individual acknowledgements or an acknowledgement for every cumulative packet ?\n",
            "0\n",
            "Go-Back-N ARQ\n",
            "Selective Repeat ARQ\n",
            "individual acknowledgements\n",
            "an acknowledgement\n",
            "every cumulative packet\n",
            "Do Go-Back-N ARQ and Selective Repeat ARQ use cumulative acknowledgements or an acknowledgement for every individual packet ?\n",
            "Go-Back-N ARQ\n",
            "Selective Repeat ARQ\n",
            "cumulative acknowledgements\n",
            "an acknowledgement\n",
            "every individual packet\n",
            "How would Democrats have reacted to Marco Rubio as the winner of the US Presidency over Hillary Clinton ?\n",
            "0\n",
            "Democrats\n",
            "Marco Rubio\n",
            "the winner\n",
            "the US Presidency\n",
            "Hillary Clinton\n",
            "How would Hillary Clinton have reacted to Democrats as the winner of the US Presidency over Marco Rubio ?\n",
            "Hillary Clinton\n",
            "Democrats\n",
            "the winner\n",
            "the US Presidency\n",
            "Marco Rubio\n",
            "Is Jakarta more advanced city ( Infra structure wise ) than Mumbai in spite of the fact that India is more developed than Indonesia ?\n",
            "0\n",
            "Jakarta more advanced city\n",
            "Infra structure\n",
            "Mumbai\n",
            "spite\n",
            "the fact\n",
            "India\n",
            "Indonesia\n",
            "Is Mumbai more developed city ( Infra structure wise ) than Jakarta in spite of the fact that Indonesia is more advanced than India ?\n",
            "Mumbai more developed city\n",
            "Infra structure\n",
            "Jakarta\n",
            "spite\n",
            "the fact\n",
            "Indonesia\n",
            "India\n",
            "What I can test in a website and public testing ? It 's an application that to be used by Android users and also , to be optimized for google . ?\n",
            "0\n",
            "What\n",
            "I\n",
            "a website\n",
            "public testing\n",
            "It\n",
            "an application\n",
            "Android users\n",
            "google\n",
            "What I can test in a website and android testing ? Its a application that to be used by public users and also , to be optimized for google . ?\n",
            "What\n",
            "I\n",
            "a website\n",
            "android testing\n",
            "Its a application\n",
            "public users\n",
            "google\n",
            "Where can I find French film or tv shows with french subtitles ?\n",
            "1\n",
            "I\n",
            "French film\n",
            "tv shows\n",
            "french subtitles\n",
            "Where can I find french film or tv shows with French subtitles ?\n",
            "I\n",
            "french film\n",
            "tv shows\n",
            "French subtitles\n",
            "What was the production of artwork intended for in Hawaii and how is it compared to the one intended for in California ?\n",
            "0\n",
            "What\n",
            "the production\n",
            "artwork\n",
            "Hawaii\n",
            "it\n",
            "California\n",
            "What was the production of artwork intended for in California and how is it compared to the one intended for in Hawaii ?\n",
            "What\n",
            "the production\n",
            "artwork\n",
            "California\n",
            "it\n",
            "Hawaii\n",
            "Is LinkedIn a good acquisition for Microsoft ? Does it make strategic sense ?\n",
            "0\n",
            "a good acquisition\n",
            "Microsoft\n",
            "it\n",
            "strategic sense\n",
            "Is LinkedIn a strategic acquisition for Microsoft ? Does it make good sense ?\n",
            "a strategic acquisition\n",
            "Microsoft\n",
            "it\n",
            "good sense\n",
            "Its 8th Jan and I am stressing out to get 95 % in 12th boards ?\n",
            "0\n",
            "I\n",
            "95 %\n",
            "12th boards\n",
            "It 's 12th Jan and I am stressing out to get 95 % in 8th boards ?\n",
            "It\n",
            "12th Jan\n",
            "I\n",
            "95 %\n",
            "8th boards\n",
            "What were the major effects of the Iquique earthquake , and how do these effects compare to the Cambodia earthquake in 1877 ?\n",
            "0\n",
            "What\n",
            "the major effects\n",
            "the Iquique earthquake\n",
            "these effects\n",
            "the Cambodia earthquake\n",
            "What were the major effects of the cambodia earthquake , and how do these effects compare to the Iquique earthquake in 1877 ?\n",
            "What\n",
            "the major effects\n",
            "the cambodia earthquake\n",
            "these effects\n",
            "the Iquique earthquake\n",
            "Why does good is bad and good is bad ?\n",
            "0\n",
            "good\n",
            "Why does bad is bad and good is good ?\n",
            "How do I master hybrid app development ? What is the best way to learn it ?\n",
            "0\n",
            "I\n",
            "hybrid app development\n",
            "What\n",
            "the best way\n",
            "it\n",
            "How do I learn hybrid app development ? What is the best way to master it ?\n",
            "I\n",
            "hybrid app development\n",
            "What\n",
            "the best way\n",
            "it\n",
            "Is it a good decision to ban currency of 500 and 1000 ? Is it really going to block all the Swiss money ? Does the decision affects black money too ?\n",
            "0\n",
            "it\n",
            "a good decision\n",
            "currency\n",
            "it\n",
            "all the Swiss money\n",
            "the decision\n",
            "black money\n",
            "Is it a good decision to ban currency of 500 and 1000 ? Is it really going to block all the black money ? Does the decision affects Swiss money too ?\n",
            "it\n",
            "a good decision\n",
            "currency\n",
            "it\n",
            "all the black money\n",
            "the decision\n",
            "Swiss money\n",
            "Why did identity politics work for Hillary Clinton but not for Obama ?\n",
            "0\n",
            "identity politics\n",
            "Hillary Clinton\n",
            "Obama\n",
            "Why did identity politics work for Obama but not for Hillary Clinton ?\n",
            "identity politics\n",
            "Obama\n",
            "Hillary Clinton\n",
            "How can one track whether read email was sent ?\n",
            "0\n",
            "email\n",
            "How can one track whether sent email was read ?\n",
            "one\n",
            "email\n",
            "What are some amazing or least known fact of Indian railway ?\n",
            "1\n",
            "What\n",
            "Indian railway\n",
            "What are least known or some amazing fact of Indian railway ?\n",
            "What\n",
            "some amazing fact\n",
            "Indian railway\n",
            "Why is India still a developing country when other developed countries like China and Japan are better colonized than India ?\n",
            "0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-83acb5d1415b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlineterminator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_bad_lines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_seperate'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_sent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;31m# tokens = tokenizer.tokenize_data(data, data_path + file_name.split('.')[0] + '_grouped')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# tokens.to_csv(data_path + file_name.split('.')[0] + '.csv')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-88d01eb3a577>\u001b[0m in \u001b[0;36mtokenize_data\u001b[0;34m(self, data, file_path, group_sent)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mends1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0mmask1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdoc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSPACY_CORE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m                 \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoun_chunks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m    437\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__call__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE003\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcomponent_cfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE005\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpipes.pyx\u001b[0m in \u001b[0;36mspacy.pipeline.pipes.Tagger.__call__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpipes.pyx\u001b[0m in \u001b[0;36mspacy.pipeline.pipes.Tagger.predict\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/thinc/neural/_classes/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mMust\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mexpected\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/thinc/neural/_classes/feed_forward.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/thinc/neural/_classes/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mMust\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mexpected\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/thinc/api.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(seqs_in)\u001b[0m\n\u001b[1;32m    308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseqs_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseqs_in\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 310\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseqs_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    311\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/thinc/neural/_classes/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mMust\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mexpected\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/thinc/neural/_classes/feed_forward.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/thinc/neural/_classes/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mMust\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mexpected\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/thinc/neural/_classes/resnet.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/thinc/neural/_classes/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mMust\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mexpected\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/thinc/neural/_classes/feed_forward.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/thinc/neural/_classes/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mMust\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mexpected\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/thinc/neural/_classes/layernorm.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchild\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_moments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0mXh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXh\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mG\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/thinc/neural/_classes/layernorm.py\u001b[0m in \u001b[0;36m_get_moments\u001b[0;34m(ops, X)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mREPRODUCE_BUG\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_get_moments_reproduce_bug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0mmu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m     \u001b[0mvar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-08\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"f\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py\u001b[0m in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims)\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0mis_float16_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mumr_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         ret = um.true_divide(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXDRWi1bBuZB",
        "colab_type": "code",
        "outputId": "89412c45-7d03-48ae-a8e3-2d05ee34885f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 589
        }
      },
      "source": [
        "data = pd.read_parquet('/content/drive/My Drive/Colab/data_PAWS_qqp/dev_seperate0.parquet')\n",
        "display(data)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>i1</th>\n",
              "      <th>s1</th>\n",
              "      <th>a1</th>\n",
              "      <th>n11</th>\n",
              "      <th>n12</th>\n",
              "      <th>m1</th>\n",
              "      <th>i2</th>\n",
              "      <th>s2</th>\n",
              "      <th>a2</th>\n",
              "      <th>n21</th>\n",
              "      <th>n22</th>\n",
              "      <th>m2</th>\n",
              "      <th>y</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[101, 1327, 1127, 1103, 1558, 3154, 1104, 1103...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[0, 2, 6, 13, 17, 0, 0, 0, 0, 0]</td>\n",
              "      <td>[1, 5, 9, 15, 20, 1, 1, 1, 1, 1]</td>\n",
              "      <td>[1, 1, 1, 1, 1, 0, 0, 0, 0, 0]</td>\n",
              "      <td>[101, 1327, 1127, 1103, 1558, 3154, 1104, 1103...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[0, 2, 6, 13, 17, 0, 0, 0, 0, 0]</td>\n",
              "      <td>[1, 5, 9, 15, 20, 1, 1, 1, 1, 1]</td>\n",
              "      <td>[1, 1, 1, 1, 1, 0, 0, 0, 0, 0]</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[101, 1109, 2564, 146, 112, 182, 4676, 1309, 6...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[0, 2, 8, 10, 13, 18, 21, 23, 24, 26]</td>\n",
              "      <td>[2, 3, 9, 11, 14, 19, 22, 24, 25, 27]</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
              "      <td>[101, 1109, 2564, 146, 112, 182, 4676, 1309, 3...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[0, 8, 10, 13, 18, 21, 23, 25, 27, 28]</td>\n",
              "      <td>[2, 9, 11, 14, 19, 22, 24, 26, 28, 29]</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[101, 1731, 1202, 146, 1294, 1139, 1207, 2179,...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[2, 4, 9, 12, 16, 18, 24, 31, 0, 0]</td>\n",
              "      <td>[3, 8, 11, 13, 17, 21, 27, 32, 1, 1]</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 0, 0]</td>\n",
              "      <td>[101, 1731, 1202, 146, 1294, 1139, 1385, 2179,...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[2, 4, 9, 12, 16, 18, 24, 31, 0, 0]</td>\n",
              "      <td>[3, 8, 11, 13, 17, 21, 27, 32, 1, 1]</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 0, 0]</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[101, 2009, 11019, 183, 112, 189, 2182, 8658, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[3, 5, 13, 18, 19, 0, 0, 0, 0, 0]</td>\n",
              "      <td>[4, 8, 17, 19, 20, 1, 1, 1, 1, 1]</td>\n",
              "      <td>[1, 1, 1, 1, 1, 0, 0, 0, 0, 0]</td>\n",
              "      <td>[101, 2009, 11019, 183, 112, 189, 2182, 8658, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[3, 5, 15, 20, 21, 0, 0, 0, 0, 0]</td>\n",
              "      <td>[4, 10, 19, 21, 22, 1, 1, 1, 1, 1]</td>\n",
              "      <td>[1, 1, 1, 1, 1, 0, 0, 0, 0, 0]</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[101, 2091, 4112, 1535, 1176, 1689, 3141, 1441...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[1, 4, 7, 0, 0, 0, 0, 0, 0, 0]</td>\n",
              "      <td>[3, 7, 13, 1, 1, 1, 1, 1, 1, 1]</td>\n",
              "      <td>[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]</td>\n",
              "      <td>[101, 2091, 1689, 3141, 1535, 1176, 4112, 1441...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[1, 5, 7, 0, 0, 0, 0, 0, 0, 0]</td>\n",
              "      <td>[4, 7, 13, 1, 1, 1, 1, 1, 1, 1]</td>\n",
              "      <td>[1, 1, 1, 0, 0, 0, 0, 0, 0, 0]</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>672</th>\n",
              "      <td>[101, 2825, 170, 2900, 1497, 7349, 1587, 1165,...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[1, 9, 12, 15, 19, 27, 0, 0, 0, 0]</td>\n",
              "      <td>[5, 10, 13, 18, 26, 28, 1, 1, 1, 1]</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 0, 0, 0, 0]</td>\n",
              "      <td>[101, 2825, 170, 2900, 1137, 1664, 118, 2900, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[1, 9, 15, 18, 21, 25, 0, 0, 0, 0]</td>\n",
              "      <td>[8, 10, 16, 19, 24, 29, 1, 1, 1, 1]</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 0, 0, 0, 0]</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>673</th>\n",
              "      <td>[101, 2421, 112, 188, 1474, 146, 1202, 183, 11...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[1, 3, 7, 14, 16, 21, 0, 0, 0, 0]</td>\n",
              "      <td>[2, 4, 9, 15, 18, 22, 1, 1, 1, 1]</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 0, 0, 0, 0]</td>\n",
              "      <td>[101, 2421, 112, 188, 1474, 146, 1225, 183, 11...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[1, 3, 7, 14, 16, 21, 0, 0, 0, 0]</td>\n",
              "      <td>[2, 4, 9, 15, 18, 22, 1, 1, 1, 1]</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 0, 0, 0, 0]</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>674</th>\n",
              "      <td>[101, 1731, 1110, 6275, 10678, 4013, 1107, 847...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[2, 6, 9, 11, 0, 0, 0, 0, 0, 0]</td>\n",
              "      <td>[4, 7, 10, 14, 1, 1, 1, 1, 1, 1]</td>\n",
              "      <td>[1, 1, 1, 1, 0, 0, 0, 0, 0, 0]</td>\n",
              "      <td>[101, 1731, 1110, 6275, 10678, 4013, 1107, 651...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[2, 6, 9, 11, 0, 0, 0, 0, 0, 0]</td>\n",
              "      <td>[4, 7, 10, 14, 1, 1, 1, 1, 1, 1]</td>\n",
              "      <td>[1, 1, 1, 1, 0, 0, 0, 0, 0, 0]</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>675</th>\n",
              "      <td>[101, 2181, 144, 24723, 2312, 12166, 1239, 178...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[1, 5, 9, 12, 16, 0, 0, 0, 0, 0]</td>\n",
              "      <td>[4, 7, 11, 14, 18, 1, 1, 1, 1, 1]</td>\n",
              "      <td>[1, 1, 1, 1, 1, 0, 0, 0, 0, 0]</td>\n",
              "      <td>[101, 2181, 144, 24723, 2312, 12166, 1239, 178...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[1, 5, 9, 12, 16, 0, 0, 0, 0, 0]</td>\n",
              "      <td>[4, 7, 11, 14, 18, 1, 1, 1, 1, 1]</td>\n",
              "      <td>[1, 1, 1, 1, 1, 0, 0, 0, 0, 0]</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>676</th>\n",
              "      <td>[101, 2825, 146, 1202, 1243, 170, 3283, 112, 1...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[1, 4, 8, 11, 15, 20, 22, 26, 30, 0]</td>\n",
              "      <td>[2, 6, 10, 14, 19, 21, 25, 29, 31, 1]</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 0]</td>\n",
              "      <td>[101, 2825, 146, 1202, 1106, 1243, 170, 3283, ...</td>\n",
              "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
              "      <td>[1, 5, 9, 12, 15, 18, 23, 25, 29, 33]</td>\n",
              "      <td>[2, 7, 11, 14, 17, 22, 24, 28, 32, 34]</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>677 rows Ã— 13 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    i1  ...  y\n",
              "0    [101, 1327, 1127, 1103, 1558, 3154, 1104, 1103...  ...  0\n",
              "1    [101, 1109, 2564, 146, 112, 182, 4676, 1309, 6...  ...  0\n",
              "2    [101, 1731, 1202, 146, 1294, 1139, 1207, 2179,...  ...  0\n",
              "3    [101, 2009, 11019, 183, 112, 189, 2182, 8658, ...  ...  0\n",
              "4    [101, 2091, 4112, 1535, 1176, 1689, 3141, 1441...  ...  0\n",
              "..                                                 ...  ... ..\n",
              "672  [101, 2825, 170, 2900, 1497, 7349, 1587, 1165,...  ...  0\n",
              "673  [101, 2421, 112, 188, 1474, 146, 1202, 183, 11...  ...  0\n",
              "674  [101, 1731, 1110, 6275, 10678, 4013, 1107, 847...  ...  0\n",
              "675  [101, 2181, 144, 24723, 2312, 12166, 1239, 178...  ...  1\n",
              "676  [101, 2825, 146, 1202, 1243, 170, 3283, 112, 1...  ...  0\n",
              "\n",
              "[677 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa1IXRK-VFgu",
        "colab_type": "text"
      },
      "source": [
        "# **BERT Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpNONuQgVot2",
        "colab_type": "text"
      },
      "source": [
        "## **Load Data**\n",
        "- TBD: Create DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nsXOxUK9PJTO",
        "colab": {}
      },
      "source": [
        "BASE_QQP_PATH = '/content/drive/My Drive/Colab/data_qqp/'\n",
        "BASE_PAWS_QQP_PATH = '/content/drive/My Drive/Colab/data_PAWS_qqp/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TiRryncwPJTs",
        "colab": {}
      },
      "source": [
        "# Load QQP sets\n",
        "data_list = [ pd.read_parquet(BASE_QQP_PATH + 'train_grouped' + str(i) + '.parquet') for i in range(0, 37) ]\n",
        "data = pd.concat(data_list)\n",
        "# data = lit_eval(data)\n",
        "train_set = data\n",
        "# display(train_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "60L05S8qPJTx",
        "colab": {}
      },
      "source": [
        "# Load PAWS QQP if generated\n",
        "if PAWS_QQP == True:    \n",
        "    data_list = [ pd.read_parquet(BASE_PAWS_QQP_PATH + 'train_grouped' + str(i) + '.parquet') for i in range(0, 2) ]\n",
        "    data = pd.concat(data_list)\n",
        "    train_set = pd.concat([train_set, data])\n",
        "\n",
        "# print(train_set.shape)\n",
        "# df = train_set.loc[train_set['y']==1]\n",
        "# print(df.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "i8SsGlBqPJT1",
        "colab": {}
      },
      "source": [
        "data_list = [ pd.read_parquet(BASE_QQP_PATH + 'dev_grouped' + str(i) + '.parquet') for i in range(0, 5) ]\n",
        "data = pd.concat(data_list)\n",
        "dev_set = data\n",
        "orig_dev_set = data\n",
        "\n",
        "\n",
        "# data = pd.read_csv(BASE_QQP_PATH + 'test.csv')\n",
        "# data['i'] = data['i'].apply(lambda x: ast.literal_eval(x))\n",
        "# data['s'] = data['s'].apply(lambda x: ast.literal_eval(x))\n",
        "# test_set = data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MRlI3U_8PJT4",
        "colab": {}
      },
      "source": [
        "paws_qqp_dev_set = None\n",
        "if PAWS_QQP == True: \n",
        "    data_list = [ pd.read_parquet(BASE_PAWS_QQP_PATH + 'dev_grouped' + str(i) + '.parquet') for i in range(0, 1) ]\n",
        "    data = pd.concat(data_list)\n",
        "    # data = lit_eval(data)\n",
        "    paws_qqp_dev_set = data\n",
        "    dev_set = pd.concat([dev_set, data])\n",
        "    # display(dev_set)\n",
        "\n",
        "\n",
        "# print(dev_set.shape)\n",
        "# df = dev_set.loc[dev_set['y']==1]\n",
        "# print(df.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtEWmkKqfa_X",
        "colab_type": "text"
      },
      "source": [
        "## DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2i6OQ9tnfZ99",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomDataset():\n",
        "    def __init__(self, data):\n",
        "        self.inputs = data['i'].values\n",
        "        self.sentence_tokens = data['s'].values\n",
        "        self.labels = data['y'].values\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return tuple([torch.tensor(self.inputs[idx]), torch.tensor(self.sentence_tokens[idx]), torch.tensor(self.labels[idx])])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIyBcOTUg1Ki",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = CustomDataset(train_set)\n",
        "dev_dataset = CustomDataset(dev_set)\n",
        "# print(dev_dataset.__getitem__(-1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HVPFZadownS",
        "colab_type": "text"
      },
      "source": [
        "## Model Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GlyEXp_0yE1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model_Classifier:\n",
        "    # initalize model\n",
        "    def __init__(self, model_class, pretrained_weights, pretrained_path=None, no_iterations=10):\n",
        "        if pretrained_path is not None:\n",
        "            self.model = model_class.from_pretrained(pretrained_path).cuda()\n",
        "        else:\n",
        "            self.model = model_class.from_pretrained(pretrained_weights).cuda()\n",
        "\n",
        "        self.TRAINING_ITERATIONS = no_iterations\n",
        "        self.WARMUP = 1000\n",
        "        self.BATCH_SIZE = 32\n",
        "        self.REPORT_FREQUENCY = 100\n",
        "        self.CHKPT_FREQUENCY = 500\n",
        "#         self.model.eval()\n",
        "\n",
        "    def train_batch(self, train_data):\n",
        "        train_ids = train_data[0].cuda()\n",
        "        train_segments = train_data[1].cuda()\n",
        "        train_labels = train_data[2].cuda()\n",
        "        # train_ids = torch.tensor(train_data['i'].values.tolist()).cuda()\n",
        "        # train_segments = torch.tensor(train_data['s'].values.tolist()).cuda()\n",
        "        # train_labels = torch.tensor(train_data['y'].values.tolist()).cuda()\n",
        "        # print(train_labels)\n",
        "\n",
        "        self.model.train()\n",
        "        # Uncomment this for BERT\n",
        "        # outputs = self.model(input_ids = train_ids, token_type_ids = train_segments, labels = train_labels)\n",
        "\n",
        "        # Uncomment this for DistilBERT\n",
        "        outputs = self.model(input_ids = train_ids, labels = train_labels)\n",
        "        loss = outputs[0].cuda()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        self.scheduler.step()\n",
        "        self.model.zero_grad()\n",
        "        return loss\n",
        "\n",
        "    # train classifier\n",
        "    def train(self, data, validation_set):\n",
        "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
        "        optimizer_grouped_parameters = [\n",
        "            {\n",
        "                \"params\": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "                \"weight_decay\": 0.0,\n",
        "            },\n",
        "            {\"params\": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n",
        "        ]\n",
        "        self.optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5, eps=1e-8)\n",
        "        # self.scheduler = get_custom_schedule(self.optimizer))\n",
        "        train_loader = torch.utils.data.DataLoader(data, batch_size=self.BATCH_SIZE, sampler=torch.utils.data.RandomSampler(data))\n",
        "        dev_loader = torch.utils.data.DataLoader(validation_set, batch_size=self.BATCH_SIZE, sampler=torch.utils.data.RandomSampler(validation_set))\n",
        "        self.scheduler = get_linear_schedule_with_warmup(self.optimizer, num_warmup_steps=len(train_loader)//10, num_training_steps=(self.TRAINING_ITERATIONS+1)*len(train_loader))\n",
        "        prev_acc = 0.0\n",
        "        for i in range(1, self.TRAINING_ITERATIONS+1):\n",
        "            count = 1\n",
        "            bar = tqdm.notebook.tqdm(total=len(train_loader))\n",
        "            for batch in train_loader:\n",
        "                train_data = batch\n",
        "                count += 1\n",
        "                loss = self.train_batch(train_data)\n",
        "                bar.update(1)\n",
        "                if count%self.CHKPT_FREQUENCY == 0:\n",
        "                    bar.write(\"%d: Loss - %s\" %(i, loss))\n",
        "                    acc = self.test(dev_loader)\n",
        "                    bar.write(\"%d: Validation - %s\" %(i, str(acc)))\n",
        "                    # if abs(prev_acc-acc) < 0.0001 and loss[0] < 0.001:\n",
        "                    #     bar.write('Finish')\n",
        "                    #     bar.close()\n",
        "                    #     self.save(directory='final')\n",
        "                    #     break\n",
        "                    prev_acc = acc\n",
        "                    bar.write(\"%d Saved\" %(i))\n",
        "                    self.save(directory='CHKPT_')\n",
        "            bar.close()\n",
        "        return\n",
        "\n",
        "    # save model\n",
        "    def save(self, directory=None):\n",
        "      if directory is not None:\n",
        "          path = '/content/drive/My Drive/Colab/' + directory + '/'\n",
        "          if not os.path.exists(path):\n",
        "              os.makedirs(path)\n",
        "          self.model.save_pretrained('/content/drive/My Drive/Colab/' + directory + '/')\n",
        "      else:\n",
        "          self.model.save_pretrained('/content/drive/My Drive/Colab/model/')\n",
        "    \n",
        "    # predict values\n",
        "    def test(self, data, is_print=False, is_loader=True, print_file_name=None, sentence_base='sentence', print_file=None):\n",
        "        dev_loader = None\n",
        "        if is_loader==True:\n",
        "            dev_loader = data\n",
        "        else:\n",
        "            dev_loader = torch.utils.data.DataLoader(data, batch_size=self.BATCH_SIZE, sampler=torch.utils.data.SequentialSampler(data))\n",
        "\n",
        "        if is_print == True and print_file_name is not None:\n",
        "            test_samples = pd.read_table(print_file_name,header=0)\n",
        "        self.model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        i = 0\n",
        "        for batch in dev_loader:\n",
        "            input_ids = batch[0].cuda()\n",
        "            input_segments = batch[1].cuda()\n",
        "            labels = batch[2]\n",
        "\n",
        "            # Uncomment this for BERT\n",
        "            # outputs = self.model(input_ids = input_ids, token_type_ids = input_segments)\n",
        "\n",
        "            # Uncomment this for DistilBERT\n",
        "            outputs = self.model(input_ids = input_ids)\n",
        "            probs = outputs[0]\n",
        "            softmax = torch.nn.functional.softmax(probs, dim=1)\n",
        "            prediction = torch.argmax(softmax, dim=1)\n",
        "            # print(prediction)\n",
        "            # print(len(prediction))\n",
        "            for j in range(len(labels)):\n",
        "                if labels[j] == prediction[j]:\n",
        "                    correct += 1\n",
        "                elif is_print==True:\n",
        "                    print_file.write(\"Sentence 1: %s \\n Sentence 2: %s \\n Label (%d) vs Prediction (%d) \\n \\n ----- \\n \\n\" %(test_samples.iloc[total][sentence_base+'1'], test_samples.iloc[total][sentence_base+'2'], labels[j], prediction[j]))\n",
        "                total += 1\n",
        "        return correct/total"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzVMYXlpk49W",
        "colab_type": "text"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKgl5Ks904oA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert_class = Model_Classifier(MODEL_CLASS, PRETRAINED, no_iterations=2, pretrained_path='/content/drive/My Drive/Colab/CHKPT_/')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQGE_PiiaSOK",
        "colab_type": "code",
        "outputId": "5cd5ed96-a5c2-4a05-ae58-062190cc6439",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 789,
          "referenced_widgets": [
            "cd78f26b426743dc9b090f067e34bac7",
            "760d52ff1ba34d59baf0c54c6b84c4e0",
            "407ac0f7d6db459fbd99e6524c6da612",
            "49b8f4f690954034bf0189b544676d61",
            "148c503473ba4ea5a28f51115027e4a4",
            "b99e7cf0fde5460b9407b49495c2444f",
            "51bd902df1df46ee9466d3f99f78bf32",
            "b56cce81e2f344ef80d460085a958b04"
          ]
        }
      },
      "source": [
        "bert_class.train(train_dataset, dev_dataset)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cd78f26b426743dc9b090f067e34bac7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=11724), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "1: Loss - tensor(0.2853, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "1: Validation - 0.8754141492886377\n",
            "1 Saved\n",
            "1: Loss - tensor(0.0966, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "1: Validation - 0.8751705320600273\n",
            "1 Saved\n",
            "1: Loss - tensor(0.3156, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "1: Validation - 0.8723202104852855\n",
            "1 Saved\n",
            "1: Loss - tensor(0.1968, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "1: Validation - 0.875048723445722\n",
            "1 Saved\n",
            "1: Loss - tensor(0.0946, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "1: Validation - 0.8776554277918535\n",
            "1 Saved\n",
            "1: Loss - tensor(0.1770, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "1: Validation - 0.8772900019489378\n",
            "1 Saved\n",
            "1: Loss - tensor(0.3710, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "1: Validation - 0.8752192555057494\n",
            "1 Saved\n",
            "1: Loss - tensor(0.2701, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "1: Validation - 0.8762911713116351\n",
            "1 Saved\n",
            "1: Loss - tensor(0.1806, device='cuda:0', grad_fn=<NllLossBackward>)\n",
            "1: Validation - 0.8731485090625609\n",
            "1 Saved\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-08e34c0efa2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbert_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-f49024b26a6f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data, validation_set)\u001b[0m\n\u001b[1;32m     58\u001b[0m                 \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                 \u001b[0mcount\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m                 \u001b[0mbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCHKPT_FREQUENCY\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-f49024b26a6f>\u001b[0m in \u001b[0;36mtrain_batch\u001b[0;34m(self, train_data)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ru_vFuEXZUKh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bert_class.save('QQP_PAWS_trained_1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjiojR2kky7F",
        "colab_type": "text"
      },
      "source": [
        "## Test Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZ7-tfteVA5H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "orig_dev_dataset = CustomDataset(orig_dev_set)\n",
        "paws_qqp_dev_dataset=None\n",
        "if PAWS_QQP == True:\n",
        "    paws_qqp_dev_dataset = CustomDataset(paws_qqp_dev_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWBhT7C6k3Hi",
        "colab_type": "code",
        "outputId": "2b2adad8-0774-4e19-c457-a883f065d4c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "bert_class = Model_Classifier(MODEL_CLASS, PRETRAINED, pretrained_path='/content/drive/My Drive/Colab/BERT_Trained/QQP_trained_1/')\n",
        "acc = bert_class.test(paws_qqp_dev_dataset, is_loader=False)\n",
        "print(\"DistilBERT Train=QQP; Test=PAWS: %lf\" %(acc))\n",
        "bert_class = Model_Classifier(MODEL_CLASS, PRETRAINED, pretrained_path='/content/drive/My Drive/Colab/BERT_Trained/QQP_PAWS_trained_1/')\n",
        "acc = bert_class.test(paws_qqp_dev_dataset, is_loader=False)\n",
        "print(\"DistilBERT Train=QQP+PAWS; Test=PAWS: %lf\" %(acc))\n",
        "\n",
        "# If you want to print the errors\n",
        "# print_file = open('/content/drive/My Drive/Colab/BERT_Trained/QQP_PAWS_trained_1/qqp_false.txt', 'w')\n",
        "# bert_class = Model_Classifier(MODEL_CLASS, PRETRAINED, pretrained_path='/content/drive/My Drive/Colab/BERT_Trained/QQP_PAWS_trained_1/')\n",
        "# acc = bert_class.test(orig_dev_dataset, is_loader=False, is_print=True, print_file_name='/content/drive/My Drive/Colab/data_qqp/dev.tsv', sentence_base='question', print_file=print_file)\n",
        "# print(\"DistilBERT 2 QQP: %lf\" %(acc))\n",
        "# print_file.close()\n",
        "# print_file = open('/content/drive/My Drive/Colab/BERT_Trained/QQP_PAWS_trained_1/qqp_paws_false.txt', 'w')\n",
        "# # bert_class = Model_Classifier(MODEL_CLASS, PRETRAINED, pretrained_path='/content/drive/My Drive/Colab/BERT_Trained/QQP_PAWS_trained_1/')\n",
        "# acc = bert_class.test(paws_qqp_dev_dataset, is_loader=False, is_print=True, print_file_name='/content/drive/My Drive/Colab/data_PAWS_qqp/dev.tsv', sentence_base='sentence', print_file=print_file)\n",
        "# print(\"DistilBERT 2 QQP PAWS: %lf\" %(acc))\n",
        "# print_file.close()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/pytorch/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DistilBERT Train=QQP; Test=PAWS: 0.317578\n",
            "DistilBERT Train=QQP+PAWS; Test=PAWS: 0.805022\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdbvnMYHVhho",
        "colab_type": "text"
      },
      "source": [
        "# **Custom Model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "796AbM1_1v1W",
        "colab_type": "text"
      },
      "source": [
        "## Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Kbk-3qRW1ujT",
        "colab": {}
      },
      "source": [
        "BASE_QQP_PATH = '/content/drive/My Drive/Colab/data_qqp/'\n",
        "BASE_PAWS_QQP_PATH = '/content/drive/My Drive/Colab/data_PAWS_qqp/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "soBK9mHD1ujX",
        "colab": {}
      },
      "source": [
        "# Load QQP sets\n",
        "data_list = [ pd.read_parquet(BASE_QQP_PATH + 'train_seperate' + str(i) + '.parquet') for i in range(0, 37) ]\n",
        "data = pd.concat(data_list)\n",
        "# data = lit_eval(data)\n",
        "train_set = data\n",
        "# display(train_set)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "68o_J6Ws1ujZ",
        "colab": {}
      },
      "source": [
        "# Load PAWS QQP if generated\n",
        "paws_train_set = None\n",
        "if PAWS_QQP == True:    \n",
        "    data_list = [ pd.read_parquet(BASE_PAWS_QQP_PATH + 'train_seperate' + str(i) + '.parquet') for i in range(0, 2) ]\n",
        "    data = pd.concat(data_list)\n",
        "    paws_train_set = data\n",
        "    train_set = pd.concat([train_set, data])\n",
        "\n",
        "# print(train_set.shape)\n",
        "# df = train_set.loc[train_set['y']==1]\n",
        "# print(df.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vmGJ1Y2W1ujb",
        "colab": {}
      },
      "source": [
        "data_list = [ pd.read_parquet(BASE_QQP_PATH + 'dev_seperate' + str(i) + '.parquet') for i in range(0, 5) ]\n",
        "data = pd.concat(data_list)\n",
        "dev_set = data\n",
        "orig_dev_set = data\n",
        "\n",
        "\n",
        "# data = pd.read_csv(BASE_QQP_PATH + 'test.csv')\n",
        "# data['i'] = data['i'].apply(lambda x: ast.literal_eval(x))\n",
        "# data['s'] = data['s'].apply(lambda x: ast.literal_eval(x))\n",
        "# test_set = data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1BiMe4y51ujd",
        "colab": {}
      },
      "source": [
        "paws_qqp_dev_set = None\n",
        "if PAWS_QQP == True: \n",
        "    data_list = [ pd.read_parquet(BASE_PAWS_QQP_PATH + 'dev_seperate' + str(i) + '.parquet') for i in range(0, 1) ]\n",
        "    data = pd.concat(data_list)\n",
        "    # data = lit_eval(data)\n",
        "    paws_qqp_dev_set = data\n",
        "    dev_set = pd.concat([dev_set, data])\n",
        "    # display(dev_set)\n",
        "\n",
        "\n",
        "# print(dev_set.shape)\n",
        "# df = dev_set.loc[dev_set['y']==1]\n",
        "# print(df.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7XoNC841z8R",
        "colab_type": "text"
      },
      "source": [
        "## Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nV5wRkfZ1a2n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CustomDataset():\n",
        "    def __init__(self, data):\n",
        "        self.inputs1 = data['i1'].values\n",
        "        self.inputs2 = data['i2'].values\n",
        "        self.sentence_tokens1 = data['s1'].values\n",
        "        self.sentence_tokens2 = data['s2'].values\n",
        "        self.attention_masks1 = data['a1'].values\n",
        "        self.attention_masks2 = data['a2'].values\n",
        "        self.starts1 = data['n11'].values\n",
        "        self.starts2 = data['n21'].values\n",
        "        self.ends1 = data['n12'].values\n",
        "        self.ends2 = data['n22'].values\n",
        "        self.masks1 = data['m1'].values\n",
        "        self.masks2 = data['m2'].values\n",
        "        self.labels = data['y'].values\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return tuple([torch.tensor(self.inputs1[idx]), torch.tensor(self.sentence_tokens1[idx]), torch.tensor(self.attention_masks1[idx]), \\\n",
        "                      torch.tensor(self.inputs2[idx]), torch.tensor(self.sentence_tokens2[idx]), torch.tensor(self.attention_masks2[idx]), \\\n",
        "                      torch.tensor(self.starts1[idx]), torch.tensor(self.ends1[idx]), torch.tensor(self.masks1[idx]), \\\n",
        "                      torch.tensor(self.starts2[idx]), torch.tensor(self.ends2[idx]), torch.tensor(self.masks2[idx]), \\\n",
        "                      torch.tensor(self.labels[idx], dtype=int)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3DiPdpk1iWe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = CustomDataset(train_set)\n",
        "dev_dataset = CustomDataset(dev_set)\n",
        "# print(dev_dataset.__getitem__(-1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ke4KiJX118S",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qxOZaVNj70V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MODEL_CLASS, TOKENIZER_CLASS, PRETRAINED = (DistilBertModel, DistilBertTokenizer, 'distilbert-base-cased')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3olZZr6cPj4",
        "colab_type": "text"
      },
      "source": [
        "### Cross_Attn_Model_with_+\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZUlWcKCtYXl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model(torch.nn.Module):\n",
        "    def __init__(self, model_class, pretrained_weights):\n",
        "        super(Model, self).__init__()\n",
        "        self.model = model_class.from_pretrained(pretrained_weights)\n",
        "\n",
        "        self.sentence_comparator = torch.nn.Sequential(torch.nn.Linear(1538, 1024), torch.nn.GELU(), torch.nn.LayerNorm(1024), torch.nn.Dropout(p=0.2))\n",
        "        self.phrase_comparator1 = torch.nn.Sequential(torch.nn.Linear(15360, 4096), torch.nn.GELU(), torch.nn.Dropout(p=0.2))\n",
        "        self.phrase_comparator2 = torch.nn.Sequential(torch.nn.Linear(4096, 1024), torch.nn.GELU(), torch.nn.LayerNorm(1024))\n",
        "        # self.phrase_comparator = torch.nn.Sequential(torch.nn.Linear(15360, 1024), torch.nn.GELU(), torch.nn.LayerNorm(1024))\n",
        "        self.comparator1 = torch.nn.Sequential(torch.nn.Linear(2048, 512), torch.nn.GELU(), torch.nn.LayerNorm(512))\n",
        "        self.comparator2 = torch.nn.Sequential(torch.nn.Linear(512, 128))\n",
        "        self.comparator3 = torch.nn.Sequential(torch.nn.Linear(128, 2), torch.nn.Softmax(dim=1))\n",
        "        self.cos_fn = torch.nn.CosineSimilarity(dim=1)\n",
        "        self.dist_fn = torch.nn.PairwiseDistance(keepdim=True)\n",
        "        self.q_linear = torch.nn.Linear(768, 768)\n",
        "        self.k_linear = torch.nn.Linear(768, 768)\n",
        "        self.v_linear = torch.nn.Linear(768, 768)\n",
        "        self.attention_head = torch.nn.MultiheadAttention(768, 8)\n",
        "        self.q_linear2 = torch.nn.Linear(768, 768)\n",
        "        self.k_linear2 = torch.nn.Linear(768, 768)\n",
        "        self.v_linear2 = torch.nn.Linear(768, 768)\n",
        "        self.attention_head2 = torch.nn.MultiheadAttention(768, 8)\n",
        "    \n",
        "    def embed_data(self, tokens, segments, attentions):\n",
        "        # Uncomment this for BERT\n",
        "        # outputs = self.model(input_ids = tokens, token_type_ids = segments)\n",
        "\n",
        "        # Uncomment this for DistilBERT\n",
        "        outputs = self.model(input_ids = tokens, attention_mask=attentions)\n",
        "        return outputs\n",
        "\n",
        "    def pool_embeds(self, embeds, other_embeds, attentions):\n",
        "        q = self.q_linear2(other_embeds)\n",
        "        k = self.k_linear2(embeds)\n",
        "        v = self.v_linear2(embeds)\n",
        "\n",
        "\n",
        "        q = q.transpose(0, 1)\n",
        "        k = k.transpose(0, 1)\n",
        "        v = v.transpose(0, 1)\n",
        "\n",
        "        new_embeds, weights = self.attention_head2(q, k, v)\n",
        "        new_embeds = new_embeds.transpose(0, 1) + embeds\n",
        "\n",
        "        masks = attentions.unsqueeze(-1).expand(embeds.size()).float()\n",
        "        sum_embeds = torch.sum(new_embeds*masks, 1)\n",
        "        normal_factor = torch.sum(masks, 1)\n",
        "        pool = sum_embeds/normal_factor\n",
        "        # print(pool.shape)\n",
        "        return pool\n",
        "\n",
        "    def pool_phrase_chunks(self, embeds, other_embeds, starts, ends, masks):\n",
        "        pools = torch.zeros((embeds.shape[0], 7680)).cuda()\n",
        "        for j in range(10):\n",
        "            filters = torch.zeros((embeds.shape[0], embeds.shape[1])).cuda()\n",
        "            for i in range(embeds.shape[0]):\n",
        "            # embed = embeds[i]\n",
        "                filters[i, starts[i][j]:ends[i][j]] = 1\n",
        "            filters = filters.unsqueeze(-1).expand(embeds.size()).float()\n",
        "            # print(filters.shape)\n",
        "\n",
        "            spl = embeds*filters\n",
        "\n",
        "            q = self.q_linear(other_embeds)\n",
        "            k = self.k_linear(embeds)\n",
        "            v = self.v_linear(spl)\n",
        "            # print(q.shape, k.shape, v.shape)\n",
        "\n",
        "            q = q.transpose(0, 1)\n",
        "            k = k.transpose(0, 1)\n",
        "            v = v.transpose(0, 1)\n",
        "\n",
        "            new_embeds, weights = self.attention_head(q, k, v)\n",
        "            new_embeds = new_embeds.transpose(0, 1) + spl\n",
        "            # print(new_embeds.shape)\n",
        "\n",
        "            sum_embeds = torch.sum(new_embeds*filters, 1)\n",
        "            normal_factor = torch.sum(filters, 1)\n",
        "            pool = sum_embeds/normal_factor\n",
        "\n",
        "            for i in range(embeds.shape[0]):\n",
        "                pools[i, 768*j:768*(j+1)] = pool[i]*masks[i][j]\n",
        "                # data = embed[starts[i][j]:ends[i][j]]\n",
        "                # sum_data = torch.sum(data, 0)\n",
        "                # normal_factor = data.shape[0]\n",
        "                # pool = sum_data/normal_factor\n",
        "                # pool = pool*masks[i][j]\n",
        "                # pools[i, 768*j:768*(j+1)] = pool\n",
        "\n",
        "        # print(pools.shape)\n",
        "        # phrase_chunks = []\n",
        "        # for chunk in chunks:\n",
        "        #     sum_embeds = torch.sum(embeds[chunks[0]:chunks[1]], 0)\n",
        "        # print(pools.shape)\n",
        "        return pools\n",
        "\n",
        "    def compare_chunk_pools(self, pools1, pools2):\n",
        "        pools = torch.cat((pools1, pools2), 1)\n",
        "        comp1 = self.phrase_comparator1(pools)\n",
        "        return self.phrase_comparator2(comp1)\n",
        "\n",
        "    def compare_sentences(self, sentence1, sentence2):\n",
        "        sentences = torch.cat((sentence1, sentence2), 1)\n",
        "        dist_euc = self.dist_fn(sentence1, sentence2)\n",
        "        sentences_fin = torch.cat((sentences, dist_euc), 1)\n",
        "        dist_cos = self.cos_fn(sentence1, sentence2).unsqueeze(-1)\n",
        "        sentences_fin2 = torch.cat((sentences_fin, dist_cos), 1)\n",
        "        # print(sentences.size())\n",
        "        return self.sentence_comparator(sentences_fin2)\n",
        "\n",
        "    def compute_siamese(self, feature_up, feature_down):\n",
        "        features = torch.cat((feature_up, feature_down), 1)\n",
        "        return self.comparator(features)\n",
        "\n",
        "    def forward(self, tokens1, tokens2, segments1, segments2, attentions1, attentions2, starts1, starts2, ends1, ends2, masks1, masks2):\n",
        "        embeds1 = self.embed_data(tokens1, segments1, attentions1)\n",
        "        embeds2 = self.embed_data(tokens2, segments2, attentions2)\n",
        "        # print(embeds1)\n",
        "        pools1 = self.pool_phrase_chunks(embeds1[0], embeds2[0], starts1, ends1, masks1)\n",
        "        pools2 = self.pool_phrase_chunks(embeds2[0], embeds1[0], starts2, ends2, masks2)\n",
        "\n",
        "        sentences1 = self.pool_embeds(embeds1[0], embeds2[0], attentions1)\n",
        "        sentences2 = self.pool_embeds(embeds2[0], embeds1[0], attentions2)\n",
        "\n",
        "        compare1 = self.compare_sentences(sentences1, sentences2)\n",
        "        compare2 = self.compare_chunk_pools(pools1, pools2)\n",
        "        # print(compare1.size())\n",
        "\n",
        "        # print(compare2.shape)\n",
        "\n",
        "        conc = torch.cat((compare1, compare2), 1)\n",
        "        res1 = self.comparator1(conc)\n",
        "        res2 = self.comparator2(res1)\n",
        "\n",
        "        # print(res1.shape)\n",
        "\n",
        "        return self.comparator3(res2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDB3oF8ycZaB",
        "colab_type": "text"
      },
      "source": [
        "### Cross_Attn_Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLrlpnTLciTj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model(torch.nn.Module):\n",
        "    def __init__(self, model_class, pretrained_weights):\n",
        "        super(Model, self).__init__()\n",
        "        self.model = model_class.from_pretrained(pretrained_weights)\n",
        "\n",
        "        self.sentence_comparator = torch.nn.Sequential(torch.nn.Linear(1538, 1024), torch.nn.GELU(), torch.nn.LayerNorm(1024), torch.nn.Dropout(p=0.2))\n",
        "        self.phrase_comparator1 = torch.nn.Sequential(torch.nn.Linear(15360, 4096), torch.nn.GELU(), torch.nn.Dropout(p=0.2))\n",
        "        self.phrase_comparator2 = torch.nn.Sequential(torch.nn.Linear(4096, 1024), torch.nn.GELU(), torch.nn.LayerNorm(1024))\n",
        "        # self.phrase_comparator = torch.nn.Sequential(torch.nn.Linear(15360, 1024), torch.nn.GELU(), torch.nn.LayerNorm(1024))\n",
        "        self.comparator1 = torch.nn.Sequential(torch.nn.Linear(2048, 512), torch.nn.GELU(), torch.nn.LayerNorm(512))\n",
        "        self.comparator2 = torch.nn.Sequential(torch.nn.Linear(512, 128))\n",
        "        self.comparator3 = torch.nn.Sequential(torch.nn.Linear(128, 2), torch.nn.Softmax(dim=1))\n",
        "        self.cos_fn = torch.nn.CosineSimilarity(dim=1)\n",
        "        self.dist_fn = torch.nn.PairwiseDistance(keepdim=True)\n",
        "        self.q_linear = torch.nn.Linear(768, 768)\n",
        "        self.k_linear = torch.nn.Linear(768, 768)\n",
        "        self.v_linear = torch.nn.Linear(768, 768)\n",
        "        self.attention_head = torch.nn.MultiheadAttention(768, 8)\n",
        "        self.q_linear2 = torch.nn.Linear(768, 768)\n",
        "        self.k_linear2 = torch.nn.Linear(768, 768)\n",
        "        self.v_linear2 = torch.nn.Linear(768, 768)\n",
        "        self.attention_head2 = torch.nn.MultiheadAttention(768, 8)\n",
        "    \n",
        "    def embed_data(self, tokens, segments, attentions):\n",
        "        # Uncomment this for BERT\n",
        "        # outputs = self.model(input_ids = tokens, token_type_ids = segments)\n",
        "\n",
        "        # Uncomment this for DistilBERT\n",
        "        outputs = self.model(input_ids = tokens, attention_mask=attentions)\n",
        "        return outputs\n",
        "\n",
        "    def pool_embeds(self, embeds, other_embeds, attentions):\n",
        "        q = self.q_linear2(other_embeds)\n",
        "        k = self.k_linear2(embeds)\n",
        "        v = self.v_linear2(embeds)\n",
        "\n",
        "\n",
        "        q = q.transpose(0, 1)\n",
        "        k = k.transpose(0, 1)\n",
        "        v = v.transpose(0, 1)\n",
        "\n",
        "        new_embeds, weights = self.attention_head2(q, k, v)\n",
        "        new_embeds = new_embeds.transpose(0, 1)\n",
        "\n",
        "        masks = attentions.unsqueeze(-1).expand(embeds.size()).float()\n",
        "        sum_embeds = torch.sum(new_embeds*masks, 1)\n",
        "        normal_factor = torch.sum(masks, 1)\n",
        "        pool = sum_embeds/normal_factor\n",
        "        # print(pool.shape)\n",
        "        return pool\n",
        "\n",
        "    def pool_phrase_chunks(self, embeds, other_embeds, starts, ends, masks):\n",
        "        pools = torch.zeros((embeds.shape[0], 7680)).cuda()\n",
        "        for j in range(10):\n",
        "            filters = torch.zeros((embeds.shape[0], embeds.shape[1])).cuda()\n",
        "            for i in range(embeds.shape[0]):\n",
        "            # embed = embeds[i]\n",
        "                filters[i, starts[i][j]:ends[i][j]] = 1\n",
        "            filters = filters.unsqueeze(-1).expand(embeds.size()).float()\n",
        "            # print(filters.shape)\n",
        "\n",
        "            spl = embeds*filters\n",
        "\n",
        "            q = self.q_linear(other_embeds)\n",
        "            k = self.k_linear(embeds)\n",
        "            v = self.v_linear(spl)\n",
        "            # print(q.shape, k.shape, v.shape)\n",
        "\n",
        "            q = q.transpose(0, 1)\n",
        "            k = k.transpose(0, 1)\n",
        "            v = v.transpose(0, 1)\n",
        "\n",
        "            new_embeds, weights = self.attention_head(q, k, v)\n",
        "            new_embeds = new_embeds.transpose(0, 1)\n",
        "            # print(new_embeds.shape)\n",
        "\n",
        "            sum_embeds = torch.sum(new_embeds*filters, 1)\n",
        "            normal_factor = torch.sum(filters, 1)\n",
        "            pool = sum_embeds/normal_factor\n",
        "\n",
        "            for i in range(embeds.shape[0]):\n",
        "                pools[i, 768*j:768*(j+1)] = pool[i]*masks[i][j]\n",
        "                # data = embed[starts[i][j]:ends[i][j]]\n",
        "                # sum_data = torch.sum(data, 0)\n",
        "                # normal_factor = data.shape[0]\n",
        "                # pool = sum_data/normal_factor\n",
        "                # pool = pool*masks[i][j]\n",
        "                # pools[i, 768*j:768*(j+1)] = pool\n",
        "\n",
        "        # print(pools.shape)\n",
        "        # phrase_chunks = []\n",
        "        # for chunk in chunks:\n",
        "        #     sum_embeds = torch.sum(embeds[chunks[0]:chunks[1]], 0)\n",
        "        # print(pools.shape)\n",
        "        return pools\n",
        "\n",
        "    def compare_chunk_pools(self, pools1, pools2):\n",
        "        pools = torch.cat((pools1, pools2), 1)\n",
        "        comp1 = self.phrase_comparator1(pools)\n",
        "        return self.phrase_comparator2(comp1)\n",
        "\n",
        "    def compare_sentences(self, sentence1, sentence2):\n",
        "        sentences = torch.cat((sentence1, sentence2), 1)\n",
        "        dist_euc = self.dist_fn(sentence1, sentence2)\n",
        "        sentences_fin = torch.cat((sentences, dist_euc), 1)\n",
        "        dist_cos = self.cos_fn(sentence1, sentence2).unsqueeze(-1)\n",
        "        sentences_fin2 = torch.cat((sentences_fin, dist_cos), 1)\n",
        "        # print(sentences.size())\n",
        "        return self.sentence_comparator(sentences_fin2)\n",
        "\n",
        "    def compute_siamese(self, feature_up, feature_down):\n",
        "        features = torch.cat((feature_up, feature_down), 1)\n",
        "        return self.comparator(features)\n",
        "\n",
        "    def forward(self, tokens1, tokens2, segments1, segments2, attentions1, attentions2, starts1, starts2, ends1, ends2, masks1, masks2):\n",
        "        embeds1 = self.embed_data(tokens1, segments1, attentions1)\n",
        "        embeds2 = self.embed_data(tokens2, segments2, attentions2)\n",
        "        # print(embeds1)\n",
        "        pools1 = self.pool_phrase_chunks(embeds1[0], embeds2[0], starts1, ends1, masks1)\n",
        "        pools2 = self.pool_phrase_chunks(embeds2[0], embeds1[0], starts2, ends2, masks2)\n",
        "\n",
        "        sentences1 = self.pool_embeds(embeds1[0], embeds2[0], attentions1)\n",
        "        sentences2 = self.pool_embeds(embeds2[0], embeds1[0], attentions2)\n",
        "\n",
        "        compare1 = self.compare_sentences(sentences1, sentences2)\n",
        "        compare2 = self.compare_chunk_pools(pools1, pools2)\n",
        "        # print(compare1.size())\n",
        "\n",
        "        # print(compare2.shape)\n",
        "\n",
        "        conc = torch.cat((compare1, compare2), 1)\n",
        "        res1 = self.comparator1(conc)\n",
        "        res2 = self.comparator2(res1)\n",
        "\n",
        "        # print(res1.shape)\n",
        "\n",
        "        return self.comparator3(res2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrJJvjJ3cb10",
        "colab_type": "text"
      },
      "source": [
        "### Self_Attn_Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjOTdOOOcsQP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model(torch.nn.Module):\n",
        "    def __init__(self, model_class, pretrained_weights):\n",
        "        super(Model, self).__init__()\n",
        "        self.model = model_class.from_pretrained(pretrained_weights)\n",
        "\n",
        "        self.sentence_comparator = torch.nn.Sequential(torch.nn.Linear(1538, 1024), torch.nn.GELU(), torch.nn.LayerNorm(1024), torch.nn.Dropout(p=0.2))\n",
        "        self.phrase_comparator1 = torch.nn.Sequential(torch.nn.Linear(15360, 4096), torch.nn.GELU(), torch.nn.Dropout(p=0.2))\n",
        "        self.phrase_comparator2 = torch.nn.Sequential(torch.nn.Linear(4096, 1024), torch.nn.GELU(), torch.nn.LayerNorm(1024))\n",
        "        # self.phrase_comparator = torch.nn.Sequential(torch.nn.Linear(15360, 1024), torch.nn.GELU(), torch.nn.LayerNorm(1024))\n",
        "        self.comparator1 = torch.nn.Sequential(torch.nn.Linear(2048, 512), torch.nn.GELU(), torch.nn.LayerNorm(512))\n",
        "        self.comparator2 = torch.nn.Sequential(torch.nn.Linear(512, 128))\n",
        "        self.comparator3 = torch.nn.Sequential(torch.nn.Linear(128, 2), torch.nn.Softmax(dim=1))\n",
        "        self.cos_fn = torch.nn.CosineSimilarity(dim=1)\n",
        "        self.dist_fn = torch.nn.PairwiseDistance(keepdim=True)\n",
        "        self.q_linear = torch.nn.Linear(768, 768)\n",
        "        self.k_linear = torch.nn.Linear(768, 768)\n",
        "        self.v_linear = torch.nn.Linear(768, 768)\n",
        "        self.attention_head = torch.nn.MultiheadAttention(768, 8)\n",
        "        self.q_linear2 = torch.nn.Linear(768, 768)\n",
        "        self.k_linear2 = torch.nn.Linear(768, 768)\n",
        "        self.v_linear2 = torch.nn.Linear(768, 768)\n",
        "        self.attention_head2 = torch.nn.MultiheadAttention(768, 8)\n",
        "    \n",
        "    def embed_data(self, tokens, segments, attentions):\n",
        "        # Uncomment this for BERT\n",
        "        # outputs = self.model(input_ids = tokens, token_type_ids = segments)\n",
        "\n",
        "        # Uncomment this for DistilBERT\n",
        "        outputs = self.model(input_ids = tokens, attention_mask=attentions)\n",
        "        return outputs\n",
        "\n",
        "    def pool_embeds(self, embeds, other_embeds, attentions):\n",
        "        q = self.q_linear2(embeds)\n",
        "        k = self.k_linear2(embeds)\n",
        "        v = self.v_linear2(embeds)\n",
        "\n",
        "\n",
        "        q = q.transpose(0, 1)\n",
        "        k = k.transpose(0, 1)\n",
        "        v = v.transpose(0, 1)\n",
        "\n",
        "        new_embeds, weights = self.attention_head2(q, k, v)\n",
        "        new_embeds = new_embeds.transpose(0, 1)\n",
        "\n",
        "        masks = attentions.unsqueeze(-1).expand(embeds.size()).float()\n",
        "        sum_embeds = torch.sum(new_embeds*masks, 1)\n",
        "        normal_factor = torch.sum(masks, 1)\n",
        "        pool = sum_embeds/normal_factor\n",
        "        # print(pool.shape)\n",
        "        return pool\n",
        "\n",
        "    def pool_phrase_chunks(self, embeds, other_embeds, starts, ends, masks):\n",
        "        pools = torch.zeros((embeds.shape[0], 7680)).cuda()\n",
        "        for j in range(10):\n",
        "            filters = torch.zeros((embeds.shape[0], embeds.shape[1])).cuda()\n",
        "            for i in range(embeds.shape[0]):\n",
        "            # embed = embeds[i]\n",
        "                filters[i, starts[i][j]:ends[i][j]] = 1\n",
        "            filters = filters.unsqueeze(-1).expand(embeds.size()).float()\n",
        "            # print(filters.shape)\n",
        "\n",
        "            spl = embeds*filters\n",
        "\n",
        "            q = self.q_linear(embeds)\n",
        "            k = self.k_linear(embeds)\n",
        "            v = self.v_linear(spl)\n",
        "            # print(q.shape, k.shape, v.shape)\n",
        "\n",
        "            q = q.transpose(0, 1)\n",
        "            k = k.transpose(0, 1)\n",
        "            v = v.transpose(0, 1)\n",
        "\n",
        "            new_embeds, weights = self.attention_head(q, k, v)\n",
        "            new_embeds = new_embeds.transpose(0, 1)\n",
        "            # print(new_embeds.shape)\n",
        "\n",
        "            sum_embeds = torch.sum(new_embeds*filters, 1)\n",
        "            normal_factor = torch.sum(filters, 1)\n",
        "            pool = sum_embeds/normal_factor\n",
        "\n",
        "            for i in range(embeds.shape[0]):\n",
        "                pools[i, 768*j:768*(j+1)] = pool[i]*masks[i][j]\n",
        "                # data = embed[starts[i][j]:ends[i][j]]\n",
        "                # sum_data = torch.sum(data, 0)\n",
        "                # normal_factor = data.shape[0]\n",
        "                # pool = sum_data/normal_factor\n",
        "                # pool = pool*masks[i][j]\n",
        "                # pools[i, 768*j:768*(j+1)] = pool\n",
        "\n",
        "        # print(pools.shape)\n",
        "        # phrase_chunks = []\n",
        "        # for chunk in chunks:\n",
        "        #     sum_embeds = torch.sum(embeds[chunks[0]:chunks[1]], 0)\n",
        "        # print(pools.shape)\n",
        "        return pools\n",
        "\n",
        "    def compare_chunk_pools(self, pools1, pools2):\n",
        "        pools = torch.cat((pools1, pools2), 1)\n",
        "        comp1 = self.phrase_comparator1(pools)\n",
        "        return self.phrase_comparator2(comp1)\n",
        "\n",
        "    def compare_sentences(self, sentence1, sentence2):\n",
        "        sentences = torch.cat((sentence1, sentence2), 1)\n",
        "        dist_euc = self.dist_fn(sentence1, sentence2)\n",
        "        sentences_fin = torch.cat((sentences, dist_euc), 1)\n",
        "        dist_cos = self.cos_fn(sentence1, sentence2).unsqueeze(-1)\n",
        "        sentences_fin2 = torch.cat((sentences_fin, dist_cos), 1)\n",
        "        # print(sentences.size())\n",
        "        return self.sentence_comparator(sentences_fin2)\n",
        "\n",
        "    def compute_siamese(self, feature_up, feature_down):\n",
        "        features = torch.cat((feature_up, feature_down), 1)\n",
        "        return self.comparator(features)\n",
        "\n",
        "    def forward(self, tokens1, tokens2, segments1, segments2, attentions1, attentions2, starts1, starts2, ends1, ends2, masks1, masks2):\n",
        "        embeds1 = self.embed_data(tokens1, segments1, attentions1)\n",
        "        embeds2 = self.embed_data(tokens2, segments2, attentions2)\n",
        "        # print(embeds1)\n",
        "        pools1 = self.pool_phrase_chunks(embeds1[0], embeds2[0], starts1, ends1, masks1)\n",
        "        pools2 = self.pool_phrase_chunks(embeds2[0], embeds1[0], starts2, ends2, masks2)\n",
        "\n",
        "        sentences1 = self.pool_embeds(embeds1[0], embeds2[0], attentions1)\n",
        "        sentences2 = self.pool_embeds(embeds2[0], embeds1[0], attentions2)\n",
        "\n",
        "        compare1 = self.compare_sentences(sentences1, sentences2)\n",
        "        compare2 = self.compare_chunk_pools(pools1, pools2)\n",
        "        # print(compare1.size())\n",
        "\n",
        "        # print(compare2.shape)\n",
        "\n",
        "        conc = torch.cat((compare1, compare2), 1)\n",
        "        res1 = self.comparator1(conc)\n",
        "        res2 = self.comparator2(res1)\n",
        "\n",
        "        # print(res1.shape)\n",
        "\n",
        "        return self.comparator3(res2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BreB-d8Tc5eQ"
      },
      "source": [
        "### Phrase_Only_Attn_Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "i5JEHNMSc5eX",
        "colab": {}
      },
      "source": [
        "class Model(torch.nn.Module):\n",
        "    def __init__(self, model_class, pretrained_weights):\n",
        "        super(Model, self).__init__()\n",
        "        self.model = model_class.from_pretrained(pretrained_weights)\n",
        "\n",
        "        self.sentence_comparator = torch.nn.Sequential(torch.nn.Linear(1538, 1024), torch.nn.GELU(), torch.nn.LayerNorm(1024), torch.nn.Dropout(p=0.2))\n",
        "        self.phrase_comparator1 = torch.nn.Sequential(torch.nn.Linear(15360, 4096), torch.nn.GELU(), torch.nn.Dropout(p=0.2))\n",
        "        self.phrase_comparator2 = torch.nn.Sequential(torch.nn.Linear(4096, 1024), torch.nn.GELU(), torch.nn.LayerNorm(1024))\n",
        "        # self.phrase_comparator = torch.nn.Sequential(torch.nn.Linear(15360, 1024), torch.nn.GELU(), torch.nn.LayerNorm(1024))\n",
        "        self.comparator1 = torch.nn.Sequential(torch.nn.Linear(2048, 512), torch.nn.GELU(), torch.nn.LayerNorm(512))\n",
        "        self.comparator2 = torch.nn.Sequential(torch.nn.Linear(512, 128))\n",
        "        self.comparator3 = torch.nn.Sequential(torch.nn.Linear(128, 2), torch.nn.Softmax(dim=1))\n",
        "        self.cos_fn = torch.nn.CosineSimilarity(dim=1)\n",
        "        self.dist_fn = torch.nn.PairwiseDistance(keepdim=True)\n",
        "        self.q_linear = torch.nn.Linear(768, 768)\n",
        "        self.k_linear = torch.nn.Linear(768, 768)\n",
        "        self.v_linear = torch.nn.Linear(768, 768)\n",
        "        self.attention_head = torch.nn.MultiheadAttention(768, 8)\n",
        "        # self.q_linear2 = torch.nn.Linear(768, 768)\n",
        "        # self.k_linear2 = torch.nn.Linear(768, 768)\n",
        "        # self.v_linear2 = torch.nn.Linear(768, 768)\n",
        "        # self.attention_head2 = torch.nn.MultiheadAttention(768, 8)\n",
        "    \n",
        "    def embed_data(self, tokens, segments, attentions):\n",
        "        # Uncomment this for BERT\n",
        "        # outputs = self.model(input_ids = tokens, token_type_ids = segments)\n",
        "\n",
        "        # Uncomment this for DistilBERT\n",
        "        outputs = self.model(input_ids = tokens, attention_mask=attentions)\n",
        "        return outputs\n",
        "\n",
        "    def pool_embeds(self, embeds, other_embeds, attentions):\n",
        "        # q = self.q_linear2(embeds)\n",
        "        # k = self.k_linear2(embeds)\n",
        "        # v = self.v_linear2(embeds)\n",
        "\n",
        "\n",
        "        # q = q.transpose(0, 1)\n",
        "        # k = k.transpose(0, 1)\n",
        "        # v = v.transpose(0, 1)\n",
        "\n",
        "        # new_embeds, weights = self.attention_head2(q, k, v)\n",
        "        # new_embeds = new_embeds.transpose(0, 1)\n",
        "\n",
        "        new_embeds = embeds\n",
        "\n",
        "        masks = attentions.unsqueeze(-1).expand(embeds.size()).float()\n",
        "        sum_embeds = torch.sum(new_embeds*masks, 1)\n",
        "        normal_factor = torch.sum(masks, 1)\n",
        "        pool = sum_embeds/normal_factor\n",
        "        # print(pool.shape)\n",
        "        return pool\n",
        "\n",
        "    def pool_phrase_chunks(self, embeds, other_embeds, starts, ends, masks):\n",
        "        pools = torch.zeros((embeds.shape[0], 7680)).cuda()\n",
        "        for j in range(10):\n",
        "            filters = torch.zeros((embeds.shape[0], embeds.shape[1])).cuda()\n",
        "            for i in range(embeds.shape[0]):\n",
        "            # embed = embeds[i]\n",
        "                filters[i, starts[i][j]:ends[i][j]] = 1\n",
        "            filters = filters.unsqueeze(-1).expand(embeds.size()).float()\n",
        "            # print(filters.shape)\n",
        "\n",
        "            spl = embeds*filters\n",
        "\n",
        "            q = self.q_linear(embeds)\n",
        "            k = self.k_linear(embeds)\n",
        "            v = self.v_linear(spl)\n",
        "            # print(q.shape, k.shape, v.shape)\n",
        "\n",
        "            q = q.transpose(0, 1)\n",
        "            k = k.transpose(0, 1)\n",
        "            v = v.transpose(0, 1)\n",
        "\n",
        "            new_embeds, weights = self.attention_head(q, k, v)\n",
        "            new_embeds = new_embeds.transpose(0, 1)\n",
        "            # print(new_embeds.shape)\n",
        "\n",
        "            sum_embeds = torch.sum(new_embeds*filters, 1)\n",
        "            normal_factor = torch.sum(filters, 1)\n",
        "            pool = sum_embeds/normal_factor\n",
        "\n",
        "            for i in range(embeds.shape[0]):\n",
        "                pools[i, 768*j:768*(j+1)] = pool[i]*masks[i][j]\n",
        "                # data = embed[starts[i][j]:ends[i][j]]\n",
        "                # sum_data = torch.sum(data, 0)\n",
        "                # normal_factor = data.shape[0]\n",
        "                # pool = sum_data/normal_factor\n",
        "                # pool = pool*masks[i][j]\n",
        "                # pools[i, 768*j:768*(j+1)] = pool\n",
        "\n",
        "        # print(pools.shape)\n",
        "        # phrase_chunks = []\n",
        "        # for chunk in chunks:\n",
        "        #     sum_embeds = torch.sum(embeds[chunks[0]:chunks[1]], 0)\n",
        "        # print(pools.shape)\n",
        "        return pools\n",
        "\n",
        "    def compare_chunk_pools(self, pools1, pools2):\n",
        "        pools = torch.cat((pools1, pools2), 1)\n",
        "        comp1 = self.phrase_comparator1(pools)\n",
        "        return self.phrase_comparator2(comp1)\n",
        "\n",
        "    def compare_sentences(self, sentence1, sentence2):\n",
        "        sentences = torch.cat((sentence1, sentence2), 1)\n",
        "        dist_euc = self.dist_fn(sentence1, sentence2)\n",
        "        sentences_fin = torch.cat((sentences, dist_euc), 1)\n",
        "        dist_cos = self.cos_fn(sentence1, sentence2).unsqueeze(-1)\n",
        "        sentences_fin2 = torch.cat((sentences_fin, dist_cos), 1)\n",
        "        # print(sentences.size())\n",
        "        return self.sentence_comparator(sentences_fin2)\n",
        "\n",
        "    def compute_siamese(self, feature_up, feature_down):\n",
        "        features = torch.cat((feature_up, feature_down), 1)\n",
        "        return self.comparator(features)\n",
        "\n",
        "    def forward(self, tokens1, tokens2, segments1, segments2, attentions1, attentions2, starts1, starts2, ends1, ends2, masks1, masks2):\n",
        "        embeds1 = self.embed_data(tokens1, segments1, attentions1)\n",
        "        embeds2 = self.embed_data(tokens2, segments2, attentions2)\n",
        "        # print(embeds1)\n",
        "        pools1 = self.pool_phrase_chunks(embeds1[0], embeds2[0], starts1, ends1, masks1)\n",
        "        pools2 = self.pool_phrase_chunks(embeds2[0], embeds1[0], starts2, ends2, masks2)\n",
        "\n",
        "        sentences1 = self.pool_embeds(embeds1[0], embeds2[0], attentions1)\n",
        "        sentences2 = self.pool_embeds(embeds2[0], embeds1[0], attentions2)\n",
        "\n",
        "        compare1 = self.compare_sentences(sentences1, sentences2)\n",
        "        compare2 = self.compare_chunk_pools(pools1, pools2)\n",
        "        # print(compare1.size())\n",
        "\n",
        "        # print(compare2.shape)\n",
        "\n",
        "        conc = torch.cat((compare1, compare2), 1)\n",
        "        res1 = self.comparator1(conc)\n",
        "        res2 = self.comparator2(res1)\n",
        "\n",
        "        # print(res1.shape)\n",
        "\n",
        "        return self.comparator3(res2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qstJCnEvc7CE"
      },
      "source": [
        "### No_Attn_Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cG6MnLePc7CJ",
        "colab": {}
      },
      "source": [
        "class Model(torch.nn.Module):\n",
        "    def __init__(self, model_class, pretrained_weights):\n",
        "        super(Model, self).__init__()\n",
        "        self.model = model_class.from_pretrained(pretrained_weights)\n",
        "\n",
        "        self.sentence_comparator = torch.nn.Sequential(torch.nn.Linear(1538, 1024), torch.nn.GELU(), torch.nn.LayerNorm(1024), torch.nn.Dropout(p=0.2))\n",
        "        self.phrase_comparator1 = torch.nn.Sequential(torch.nn.Linear(15360, 4096), torch.nn.GELU(), torch.nn.Dropout(p=0.2))\n",
        "        self.phrase_comparator2 = torch.nn.Sequential(torch.nn.Linear(4096, 1024), torch.nn.GELU(), torch.nn.LayerNorm(1024))\n",
        "        # self.phrase_comparator = torch.nn.Sequential(torch.nn.Linear(15360, 1024), torch.nn.GELU(), torch.nn.LayerNorm(1024))\n",
        "        self.comparator1 = torch.nn.Sequential(torch.nn.Linear(2048, 512), torch.nn.GELU(), torch.nn.LayerNorm(512))\n",
        "        self.comparator2 = torch.nn.Sequential(torch.nn.Linear(512, 128))\n",
        "        self.comparator3 = torch.nn.Sequential(torch.nn.Linear(128, 2), torch.nn.Softmax(dim=1))\n",
        "        self.cos_fn = torch.nn.CosineSimilarity(dim=1)\n",
        "        self.dist_fn = torch.nn.PairwiseDistance(keepdim=True)\n",
        "    \n",
        "    def embed_data(self, tokens, segments, attentions):\n",
        "        # Uncomment this for BERT\n",
        "        # outputs = self.model(input_ids = tokens, token_type_ids = segments)\n",
        "\n",
        "        # Uncomment this for DistilBERT\n",
        "        outputs = self.model(input_ids = tokens, attention_mask=attentions)\n",
        "        return outputs\n",
        "\n",
        "    def pool_embeds(self, embeds, other_embeds, attentions):\n",
        "        masks = attentions.unsqueeze(-1).expand(embeds.size()).float()\n",
        "        sum_embeds = torch.sum(embeds*masks, 1)\n",
        "        normal_factor = torch.sum(masks, 1)\n",
        "        pool = sum_embeds/normal_factor\n",
        "        # print(pool.shape)\n",
        "        return pool\n",
        "\n",
        "    def pool_phrase_chunks(self, embeds, other_embeds, starts, ends, masks):\n",
        "        pools = torch.zeros((embeds.shape[0], 7680)).cuda()\n",
        "        for j in range(10):\n",
        "            filters = torch.zeros((embeds.shape[0], embeds.shape[1])).cuda()\n",
        "            for i in range(embeds.shape[0]):\n",
        "            # embed = embeds[i]\n",
        "                filters[i, starts[i][j]:ends[i][j]] = 1\n",
        "            filters = filters.unsqueeze(-1).expand(embeds.size()).float()\n",
        "            # print(filters.shape)\n",
        "\n",
        "            spl = embeds*filters\n",
        "            new_embeds = spl\n",
        "            # print(new_embeds.shape)\n",
        "\n",
        "            sum_embeds = torch.sum(new_embeds*filters, 1)\n",
        "            normal_factor = torch.sum(filters, 1)\n",
        "            pool = sum_embeds/normal_factor\n",
        "\n",
        "            for i in range(embeds.shape[0]):\n",
        "                pools[i, 768*j:768*(j+1)] = pool[i]*masks[i][j]\n",
        "                # data = embed[starts[i][j]:ends[i][j]]\n",
        "                # sum_data = torch.sum(data, 0)\n",
        "                # normal_factor = data.shape[0]\n",
        "                # pool = sum_data/normal_factor\n",
        "                # pool = pool*masks[i][j]\n",
        "                # pools[i, 768*j:768*(j+1)] = pool\n",
        "\n",
        "        # print(pools.shape)\n",
        "        # phrase_chunks = []\n",
        "        # for chunk in chunks:\n",
        "        #     sum_embeds = torch.sum(embeds[chunks[0]:chunks[1]], 0)\n",
        "        # print(pools.shape)\n",
        "        return pools\n",
        "\n",
        "    def compare_chunk_pools(self, pools1, pools2):\n",
        "        pools = torch.cat((pools1, pools2), 1)\n",
        "        comp1 = self.phrase_comparator1(pools)\n",
        "        return self.phrase_comparator2(comp1)\n",
        "\n",
        "    def compare_sentences(self, sentence1, sentence2):\n",
        "        sentences = torch.cat((sentence1, sentence2), 1)\n",
        "        dist_euc = self.dist_fn(sentence1, sentence2)\n",
        "        sentences_fin = torch.cat((sentences, dist_euc), 1)\n",
        "        dist_cos = self.cos_fn(sentence1, sentence2).unsqueeze(-1)\n",
        "        sentences_fin2 = torch.cat((sentences_fin, dist_cos), 1)\n",
        "        # print(sentences.size())\n",
        "        return self.sentence_comparator(sentences_fin2)\n",
        "\n",
        "    def compute_siamese(self, feature_up, feature_down):\n",
        "        features = torch.cat((feature_up, feature_down), 1)\n",
        "        return self.comparator(features)\n",
        "\n",
        "    def forward(self, tokens1, tokens2, segments1, segments2, attentions1, attentions2, starts1, starts2, ends1, ends2, masks1, masks2):\n",
        "        embeds1 = self.embed_data(tokens1, segments1, attentions1)\n",
        "        embeds2 = self.embed_data(tokens2, segments2, attentions2)\n",
        "        # print(embeds1)\n",
        "        pools1 = self.pool_phrase_chunks(embeds1[0], embeds2[0], starts1, ends1, masks1)\n",
        "        pools2 = self.pool_phrase_chunks(embeds2[0], embeds1[0], starts2, ends2, masks2)\n",
        "\n",
        "        sentences1 = self.pool_embeds(embeds1[0], embeds2[0], attentions1)\n",
        "        sentences2 = self.pool_embeds(embeds2[0], embeds1[0], attentions2)\n",
        "\n",
        "        compare1 = self.compare_sentences(sentences1, sentences2)\n",
        "        compare2 = self.compare_chunk_pools(pools1, pools2)\n",
        "        # print(compare1.size())\n",
        "\n",
        "        # print(compare2.shape)\n",
        "\n",
        "        conc = torch.cat((compare1, compare2), 1)\n",
        "        res1 = self.comparator1(conc)\n",
        "        res2 = self.comparator2(res1)\n",
        "\n",
        "        # print(res1.shape)\n",
        "\n",
        "        return self.comparator3(res2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t40VYaopc0n4",
        "colab_type": "text"
      },
      "source": [
        "### Model Util"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h355NpHZ0zv0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model_util():\n",
        "    # initalize model\n",
        "    def __init__(self, model_class, pretrained_weights, pretrained_path=None, no_iterations=10):\n",
        "        self.model = Model(model_class, pretrained_weights)\n",
        "        if pretrained_path is not None:\n",
        "            self.model.load_state_dict(torch.load(pretrained_path))\n",
        "        \n",
        "        self.model = self.model.cuda()\n",
        "        self.loss_func = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "        self.TRAINING_ITERATIONS = no_iterations\n",
        "        self.WARMUP = 1000\n",
        "        self.BATCH_SIZE = 32\n",
        "        self.REPORT_FREQUENCY = 100\n",
        "        self.CHKPT_FREQUENCY = 1000\n",
        "#         self.model.eval()\n",
        "\n",
        "    def train_batch(self, train_data):\n",
        "        labels = train_data[12].cuda()\n",
        "        tokens1 = train_data[0].cuda()\n",
        "        tokens2 = train_data[3].cuda()\n",
        "        segments1 = train_data[1].cuda()\n",
        "        segments2 = train_data[4].cuda()\n",
        "        attentions1 = train_data[2].cuda()\n",
        "        attentions2 = train_data[5].cuda()\n",
        "        starts1 = train_data[6].cuda()\n",
        "        ends1 = train_data[7].cuda()\n",
        "        masks1 = train_data[8].cuda()\n",
        "        starts2 = train_data[9].cuda()\n",
        "        ends2 = train_data[10].cuda()\n",
        "        masks2 = train_data[11].cuda()\n",
        "        \n",
        "        self.model.train()\n",
        "        outputs = self.model(tokens1, tokens2, segments1, segments2, attentions1, attentions2, starts1, starts2, ends1, ends2, masks1, masks2)\n",
        "        loss = self.loss_func(outputs, labels)\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        # self.scheduler.step()\n",
        "        self.model.zero_grad()\n",
        "        return loss\n",
        "\n",
        "    # train classifier\n",
        "    def train(self, data, validation_set):\n",
        "        self.optimizer = torch.optim.AdamW(self.model.parameters(), lr=1e-5, eps=1e-8)\n",
        "        # self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, verbose=True)\n",
        "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, 1, 0.9)\n",
        "        train_loader = torch.utils.data.DataLoader(data, batch_size=self.BATCH_SIZE, sampler=torch.utils.data.RandomSampler(data))\n",
        "        dev_loader = torch.utils.data.DataLoader(validation_set, batch_size=self.BATCH_SIZE, sampler=torch.utils.data.RandomSampler(validation_set))\n",
        "\n",
        "        prev_acc = 0.0\n",
        "        for i in range(1, self.TRAINING_ITERATIONS+1):\n",
        "            count = 0\n",
        "            # bar = tqdm.notebook.tqdm(total=len(train_loader))\n",
        "            for batch in train_loader:\n",
        "                train_data = batch\n",
        "                count += 1\n",
        "                loss = self.train_batch(train_data)\n",
        "                # print(loss)\n",
        "                # break\n",
        "                # bar.update(1)\n",
        "                # break\n",
        "                if count%self.REPORT_FREQUENCY == 0:\n",
        "                    print(\"%d: Loss - %s\" %(count, loss))\n",
        "                # if count%self.CHKPT_FREQUENCY == 0:\n",
        "                    # bar.write(\"%d: Loss - %s\" %(i, loss))\n",
        "            acc = self.test(dev_loader)\n",
        "            print(\"%d: Validation - %s\" %(count, str(acc)))\n",
        "            prev_acc = acc\n",
        "            self.save(directory='CHKPT_')\n",
        "            print(\"%d Saved\" %(count))\n",
        "            self.scheduler.step()\n",
        "            # bar.close()\n",
        "        return\n",
        "\n",
        "    # save model\n",
        "    def save(self, directory=None):\n",
        "      if directory is not None:\n",
        "          path = '/content/drive/My Drive/Colab/' + directory\n",
        "          if not os.path.exists(path):\n",
        "              os.makedirs(path)\n",
        "          torch.save(self.model.state_dict(), path + '/1.pt')\n",
        "      else:\n",
        "          torch.save(self.model.state_dict(), '/content/drive/My Drive/Colab/model/1.pt')\n",
        "    \n",
        "    # predict values\n",
        "    def test(self, data, is_print=False, is_loader=True, print_file_name=None, sentence_base='sentence', print_file=None):\n",
        "        dev_loader = None\n",
        "        if is_loader==True:\n",
        "            dev_loader = data\n",
        "        else:\n",
        "            dev_loader = torch.utils.data.DataLoader(data, batch_size=self.BATCH_SIZE, sampler=torch.utils.data.SequentialSampler(data))\n",
        "\n",
        "        if is_print == True and print_file_name is not None:\n",
        "            test_samples = pd.read_table(print_file_name,header=0)\n",
        "        self.model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        i = 0\n",
        "        for batch in dev_loader:\n",
        "            labels = batch[12]\n",
        "            tokens1 = batch[0].cuda()\n",
        "            tokens2 = batch[3].cuda()\n",
        "            segments1 = batch[1].cuda()\n",
        "            segments2 = batch[4].cuda()\n",
        "            attentions1 = batch[2].cuda()\n",
        "            attentions2 = batch[5].cuda()\n",
        "            starts1 = batch[6].cuda()\n",
        "            ends1 = batch[7].cuda()\n",
        "            masks1 = batch[8].cuda()\n",
        "            starts2 = batch[9].cuda()\n",
        "            ends2 = batch[10].cuda()\n",
        "            masks2 = batch[11].cuda()\n",
        "\n",
        "            outputs = self.model(tokens1, tokens2, segments1, segments2, attentions1, attentions2, starts1, starts2, ends1, ends2, masks1, masks2)\n",
        "            probs = outputs.cpu()\n",
        "            prediction = torch.argmax(probs, dim=1)\n",
        "            # prediction = np.zeros(probs.size()[0])\n",
        "            # for j in range(len(prediction)):\n",
        "            #     if probs[j] > 0.5:\n",
        "            #         prediction[j] = 1\n",
        "            for j in range(len(labels)):\n",
        "                if labels[j] == prediction[j]:\n",
        "                    correct += 1\n",
        "                elif is_print==True:\n",
        "                    print_file.write(\"Sentence 1: %s \\n Sentence 2: %s \\n Label (%d) vs Prediction (%d) \\n \\n ----- \\n \\n\" %(test_samples.iloc[total][sentence_base+'1'], test_samples.iloc[total][sentence_base+'2'], labels[j], prediction[j]))\n",
        "                total += 1\n",
        "        return correct/total"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3tBA6Or1soN",
        "colab_type": "text"
      },
      "source": [
        "### Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUtjhFvt8WZU",
        "colab_type": "code",
        "outputId": "71de468e-2367-4e78-faa1-6e16661dce1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "classifier = Model_util(MODEL_CLASS, PRETRAINED, no_iterations=10, pretrained_path='/content/drive/My Drive/Colab/Custom_Trained/Attn_Both_Cross_+.pt')\n",
        "classifier.train(train_dataset, dev_dataset)\n",
        "# classifier.train(CustomDataset(paws_train_set), CustomDataset(paws_qqp_dev_set))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-f3d62adcf1cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel_util\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMODEL_CLASS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPRETRAINED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/drive/My Drive/Colab/Custom_Trained/Attn_Both_Cross_+.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# classifier.train(train_dataset, dev_dataset)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCustomDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaws_train_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCustomDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaws_qqp_dev_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-048defb4a509>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_class, pretrained_weights, pretrained_path, no_iterations)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# initalize model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpretrained_path\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-b3a3ac79b524>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_class, pretrained_weights)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence_comparator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1538\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGELU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0;31m# Instantiate model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 512\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstate_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfrom_tf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_distilbert.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_input_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36minit_weights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;34m\"\"\" Initialize and prunes weights if needed. \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;31m# Initialize weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m         \u001b[0;31m# Prune heads if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \"\"\"\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m         \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \"\"\"\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m         \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \"\"\"\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m         \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \"\"\"\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m         \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \"\"\"\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m         \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m         \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    291\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_distilbert.py\u001b[0m in \u001b[0;36m_init_weights\u001b[0;34m(self, module)\u001b[0m\n\u001b[1;32m    339\u001b[0m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitializer_range\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitializer_range\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iM8OCQw1voa",
        "colab_type": "text"
      },
      "source": [
        "### Test Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HH3fmUhZkQ_k",
        "colab_type": "code",
        "outputId": "445f3caf-5e26-4948-8265-55c508a17cc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "classifier = Model_util(MODEL_CLASS, PRETRAINED, pretrained_path='/content/drive/My Drive/Colab/Custom_Trained/Attn_Both_Cross_+.pt')\n",
        "# classifier = Model_util(MODEL_CLASS, PRETRAINED, pretrained_path='/content/drive/My Drive/Colab/CHKPT_/1.pt')\n",
        "test_data = CustomDataset(paws_qqp_dev_set)\n",
        "# acc = classifier.test(test_data, is_loader=False)\n",
        "acc = classifier.test(test_data, is_loader=False, print_file_name='/content/drive/My Drive/Colab/data_PAWS_qqp/dev.tsv', print_file=open('/content/drive/My Drive/Colab/temp.txt', 'w'), is_print=True)\n",
        "print(\"HERE1\")\n",
        "print(\"PAWS_QQP test set: %lf\" %(acc))\n",
        "print(\"HERE2\")\n",
        "acc = classifier.test(dev_dataset, is_loader=False)\n",
        "print(\"Full test set: %lf\" %(acc))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/pytorch/torch/csrc/utils/tensor_numpy.cpp:141: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "HERE1\n",
            "PAWS_QQP test set: 0.577548\n",
            "HERE2\n",
            "Full test set: 0.872613\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}