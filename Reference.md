# Reference

## Papers, Blogs about BERT

##### some resources about BERT -- Yuanyuan, 20200309:

BERT, or Bidirectional Encoder Representations from Transformers, is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks.

###### (1) Intro

https://www.blog.google/products/search/search-language-understanding-bert/

https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html

A super great blog about NLP: FROM Pre-trained Word Embeddings TO Pre-trained Language Models — Focus on BERT
Paper: https://towardsdatascience.com/from-pre-trained-word-embeddings-to-pre-trained-language-models-focus-on-bert-343815627598

The academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: https://arxiv.org/abs/1810.04805.


###### (2) Official

Github: https://github.com/google-research/bert

Google Colab:BERT End to End (Fine-tuning + Predicting) with Cloud TPU: Sentence and Sentence-Pair Classification Tasks

The popular multilingual BERT model: https://github.com/google-research/bert/blob/master/multilingual.md

###### (3) Blogs 

https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb#scrollTo=wHQH4OCHZ9bq

*Tips: If you get a message as “You’ve reached the end of your free member preview for this month.” from https://medium.com/, read those blogs in the incognito mode of your browser.

Google’s BERT changing the NLP Landscape: https://medium.com/sciforce/googles-bert-changing-the-nlp-landscape-5f4a7bf65cc5

A Visual Guide to Using BERT for the First Time: http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/

BERT for dummies — Step by Step Tutorial: https://towardsdatascience.com/bert-for-dummies-step-by-step-tutorial-fb90890ffe03

State-of-the-art pre-training for natural language processing with BERT: https://blog.insightdatascience.com/using-bert-for-state-of-the-art-pre-training-for-natural-language-processing-1d87142c29e7

A Hands-On Guide To Text Classification With Transformer Models (XLNet, BERT, XLM, RoBERTa): https://towardsdatascience.com/https-medium-com-chaturangarajapakshe-text-classification-with-transformer-models-d370944b50ca (github: https://github.com/ThilinaRajapakse/pytorch-transformers-classification) (out of date, no need to use)

Simple Transformers — Introducing The Easiest Way To Use BERT, RoBERTa, XLNet, and XLM: https://towardsdatascience.com/simple-transformers-introducing-the-easiest-bert-roberta-xlnet-and-xlm-library-58bf8c59b2a3 (github: https://github.com/ThilinaRajapakse/simpletransformers)

Question Answering with BERT, XLNET, XLM, and DistilBERT using Simple Transformers: https://towardsdatascience.com/question-answering-with-bert-xlnet-xlm-and-distilbert-using-simple-transformers-4d8785ee762a
